{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94ade1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import your existing RAG fusion function\n",
    "from rag_pipeline.rag_fusion_pipeline import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SmartRAGTool:\n",
    "    def __init__(self, \n",
    "                 local_index_path: str,\n",
    "                 embedding_model,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Smart RAG Tool\n",
    "        \n",
    "        Args:\n",
    "            local_index_path: Path to the FAISS vector store\n",
    "            embedding_model: Embedding model for vector store\n",
    "            llm_params: Parameters for the LLM\n",
    "        \"\"\"\n",
    "        self.local_index_path = local_index_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "        self.llm = ChatOpenAI(**self.llm_params)\n",
    "        \n",
    "        # Function definition for OpenAI function calling\n",
    "        self.function_definition = {\n",
    "            \"name\": \"search_knowledge_base\",\n",
    "            \"description\": \"Search the knowledge base for specific information when the question requires domain-specific or detailed factual information that may not be in general knowledge\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to find relevant information\"\n",
    "                    },\n",
    "                    \"mode\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"original\", \"generated\"],\n",
    "                        \"description\": \"Search mode: 'original' uses only the user query, 'generated' creates multiple related queries for better coverage\"\n",
    "                    },\n",
    "                    \"num_queries\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"minimum\": 1,\n",
    "                        \"maximum\": 10,\n",
    "                        \"description\": \"Number of queries to generate if using 'generated' mode (default: 3)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Prompt to determine if RAG is needed\n",
    "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an AI assistant that decides whether a user question requires searching a knowledge base or can be answered with general knowledge.\n",
    "\n",
    "Use the search_knowledge_base function ONLY when:\n",
    "1. The questions is specific and realted to a policy, fact, legal specification.\n",
    "2. The question requires current or specific information that might not be in general knowledge\n",
    "\n",
    "DO NOT use the search function for:\n",
    "1. General knowledge questions (e.g., \"What is machine learning?\", \"How does photosynthesis work?\")\n",
    "2. Questions not related to the knowledge questions should be politely redirected to ask user for questions that are related to the knowledge base.\n",
    "\n",
    "If you decide to search, choose the appropriate mode:\n",
    "- Use \"original\" mode for simple, direct queries\n",
    "- Use \"generated\" mode for complex questions that might benefit from multiple search perspectives\n",
    "\n",
    "If you don't need to search, answer the question directly using your general knowledge.\"\"\"),\n",
    "            (\"user\", \"{user_query}\")\n",
    "        ])\n",
    "\n",
    "    def search_knowledge_base(self, query: str, mode: str = \"generated\", num_queries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Search the knowledge base using RAG fusion\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            mode: Search mode ('original' or 'generated')\n",
    "            num_queries: Number of queries to generate if using 'generated' mode\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Searching knowledge base with query: '{query}' in {mode} mode\")\n",
    "            \n",
    "            answer, metadata = rag_fusion_answer(\n",
    "                user_query=query,\n",
    "                local_index_path=self.local_index_path,\n",
    "                embedding_model=self.embedding_model,\n",
    "                mode=mode,\n",
    "                num_generated_queries=num_queries,\n",
    "                top_k=5,  # Retrieve more documents for better context\n",
    "                params=self.llm_params\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"metadata\": metadata,\n",
    "                \"search_performed\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in knowledge base search: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": f\"I encountered an error while searching the knowledge base: {str(e)}\",\n",
    "                \"metadata\": {},\n",
    "                \"search_performed\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def process_user_query(self, user_query: str, chat_context: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a user query and decide whether to use RAG or answer directly\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            chat_context: Optional conversation context\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (answer, metadata)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create the chain with function calling\n",
    "            chain = self.decision_prompt | self.llm.bind(\n",
    "                functions=[self.function_definition],\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "            \n",
    "            metadata = {\n",
    "                \"user_query\": user_query,\n",
    "                \"decision_made\": None,\n",
    "                \"search_performed\": False,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"token_usage\": {}\n",
    "            }\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain.invoke({\"user_query\": user_query})\n",
    "            \n",
    "            # Track decision-making cost\n",
    "            decision_cost = {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "            metadata[\"decision_cost\"] = decision_cost\n",
    "            metadata[\"total_cost\"] += cb.total_cost\n",
    "            \n",
    "            # Check if the model decided to use function calling\n",
    "            if hasattr(response, 'additional_kwargs') and 'function_call' in response.additional_kwargs:\n",
    "                function_call = response.additional_kwargs['function_call']\n",
    "                function_name = function_call['name']\n",
    "                function_args = json.loads(function_call['arguments'])\n",
    "                \n",
    "                logger.info(f\"LLM decided to use function: {function_name} with args: {function_args}\")\n",
    "                metadata[\"decision_made\"] = \"search_needed\"\n",
    "                \n",
    "                if function_name == \"search_knowledge_base\":\n",
    "                    # Execute the RAG search\n",
    "                    search_result = self.search_knowledge_base(\n",
    "                        query=function_args.get('query', user_query),\n",
    "                        mode=function_args.get('mode', 'generated'),\n",
    "                        num_queries=function_args.get('num_queries', 3)\n",
    "                    )\n",
    "                    \n",
    "                    if search_result.get(\"search_performed\"):\n",
    "                        metadata.update(search_result[\"metadata\"])\n",
    "                        metadata[\"search_performed\"] = True\n",
    "                        metadata[\"total_cost\"] += search_result[\"metadata\"].get(\"total_price\", 0)\n",
    "                        \n",
    "                        return search_result[\"answer\"], metadata\n",
    "                    else:\n",
    "                        # Fallback if search failed\n",
    "                        return f\"I tried to search for information but encountered an issue. Based on general knowledge: I'd be happy to help, but I may need more specific information to give you the most accurate answer.\", metadata\n",
    "                        \n",
    "            else:\n",
    "                # LLM decided not to search - use the direct response\n",
    "                logger.info(\"LLM decided no search needed, providing direct answer\")\n",
    "                metadata[\"decision_made\"] = \"direct_answer\"\n",
    "                return response.content, metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_user_query: {str(e)}\")\n",
    "            metadata[\"error\"] = str(e)\n",
    "            return f\"I encountered an error while processing your question: {str(e)}\", metadata\n",
    "\n",
    "    def chat(self, user_query: str, chat_context: Optional[str] = None, verbose: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Simple chat interface that handles the query and returns just the answer\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            chat_context: Optional conversation context\n",
    "            verbose: Whether to print detailed metadata\n",
    "            \n",
    "        Returns:\n",
    "            The answer string\n",
    "        \"\"\"\n",
    "        answer, metadata = self.process_user_query(user_query, chat_context)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Smart RAG Tool Execution Report ---\")\n",
    "            print(f\"User Query: {user_query}\")\n",
    "            print(f\"Decision Made: {metadata.get('decision_made', 'unknown')}\")\n",
    "            print(f\"Search Performed: {metadata.get('search_performed', False)}\")\n",
    "            print(f\"Total Cost: ${metadata.get('total_cost', 0):.4f}\")\n",
    "            \n",
    "            if metadata.get('search_performed'):\n",
    "                token_usage = metadata.get('token_usage', {})\n",
    "                print(f\"Total Tokens Used: {token_usage.get('total_tokens', 0)}\")\n",
    "                print(f\"Queries Used: {metadata.get('queries_used', [])}\")\n",
    "                print(f\"Documents Retrieved: {metadata.get('num_documents_retrieved', 0)}\")\n",
    "            \n",
    "            print(f\"--- End Report ---\\n\")\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "def create_smart_rag_tool(local_index_path: str, embedding_model) -> SmartRAGTool:\n",
    "    \"\"\"Factory function to create a Smart RAG Tool instance\"\"\"\n",
    "    return SmartRAGTool(\n",
    "        local_index_path=local_index_path,\n",
    "        embedding_model=embedding_model,\n",
    "        llm_params={\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "    # Initialize the tool\n",
    "    # smart_rag = create_smart_rag_tool(\"./faiss_index\", your_embedding_model)\n",
    "    \n",
    "    # Example questions that would trigger different behaviors:\n",
    "    \n",
    "    # This would likely NOT trigger RAG (general knowledge)\n",
    "    # answer = smart_rag.chat(\"What is machine learning?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # This would likely trigger RAG (specific/domain knowledge)\n",
    "    # answer = smart_rag.chat(\"What are the specific implementation details of our authentication system?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # This would likely trigger RAG with generated mode (complex query)\n",
    "    # answer = smart_rag.chat(\"How does our system handle user permissions and what are the security implications?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7274cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "smart_rag = create_smart_rag_tool(\"./data/faiss_index\", OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided to use function: search_knowledge_base with args: {'query': 'пороговый уровень ОЗП', 'mode': 'original'}\n",
      "INFO:__main__:Searching knowledge base with query: 'пороговый уровень ОЗП' in original mode\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "INFO:rag_pipeline.rag_fusion_pipeline:Running in original query mode.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart RAG Tool Execution Report ---\n",
      "User Query: Какой пороговый уровень ОЗП\n",
      "Decision Made: search_needed\n",
      "Search Performed: True\n",
      "Total Cost: $0.0065\n",
      "Total Tokens Used: 1720\n",
      "Queries Used: ['пороговый уровень ОЗП']\n",
      "Documents Retrieved: 4\n",
      "--- End Report ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Пороговый уровень для оценки знаний педагогов (ОЗП) зависит от квалификационной категории педагога. Согласно предоставленным документам, пороговые уровни следующие:\\n\\n- Для квалификационной категории «педагог-стажер/педагог» - 50%;\\n- Для квалификационной категории «педагог-модератор» - 60%;\\n- Для квалификационной категории «педагог-эксперт» - 70%;\\n- Для квалификационной категории «педагог-исследователь» - 80%;\\n- Для квалификационной категории «педагог-мастер» - 90%.\\n\\nДля первых руководителей, заместителей руководителя организаций образования и методических кабинетов (центров) пороговый уровень составляет 70%.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = smart_rag.chat(\"Какой пороговый уровень ОЗП\", verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe0a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided no search needed, providing direct answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart RAG Tool Execution Report ---\n",
      "User Query: Какая столица Франции?\n",
      "Decision Made: direct_answer\n",
      "Search Performed: False\n",
      "Total Cost: $0.0009\n",
      "--- End Report ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Столица Франции — Париж.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_rag.chat(\"Какая столица Франции?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFRetrievalTool:\n",
    "    def __init__(self, \n",
    "                 documents_json_path: str,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the PDF Retrieval Tool\n",
    "        \n",
    "        Args:\n",
    "            documents_json_path: Path to JSON file containing document mappings\n",
    "            llm_params: Parameters for the LLM\n",
    "        \"\"\"\n",
    "        self.documents_json_path = documents_json_path\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "        self.llm = ChatOpenAI(**self.llm_params)\n",
    "        \n",
    "        # Load document mappings\n",
    "        self.document_mappings = self._load_document_mappings()\n",
    "        \n",
    "        # Function definition for OpenAI function calling\n",
    "        self.function_definition = {\n",
    "            \"name\": \"retrieve_document\",\n",
    "            \"description\": \"Retrieve a specific PDF document by name when the user explicitly asks for a document, manual, guide, or file\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"document_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name or identifier of the document to retrieve\"\n",
    "                    },\n",
    "                    \"search_terms\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Alternative search terms or keywords to find the document if exact name doesn't match\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"document_name\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Prompt to determine if document retrieval is needed\n",
    "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are an AI assistant that determines if a user is asking for a specific document/PDF file.\n",
    "\n",
    "Available documents:\n",
    "{self._format_available_documents()}\n",
    "\n",
    "Use the retrieve_document function ONLY when:\n",
    "1. The user explicitly asks for a document, manual, guide, report, or PDF\n",
    "2. The user mentions wanting to \"download\", \"get\", \"retrieve\", or \"access\" a specific file\n",
    "3. The user asks for documentation about a specific topic that matches our available documents\n",
    "\n",
    "Examples of when to use retrieve_document:\n",
    "- \"Can I get the user manual?\"\n",
    "- \"I need the API documentation\"\n",
    "- \"Download the installation guide\"\n",
    "- \"Show me the quarterly report\"\n",
    "- \"I want to see the policy document\"\n",
    "\n",
    "Examples of when NOT to use retrieve_document:\n",
    "- \"Tell me about our API\" (information request, not document request)\n",
    "- \"How do I install the software?\" (question, not document request)\n",
    "- \"What's in the quarterly report?\" (asking for summary, not the document itself)\n",
    "\n",
    "When calling the function, try to match the user's request to the most appropriate document name from the available list. If unsure, include relevant search terms.\"\"\"),\n",
    "            (\"user\", \"{user_query}\")\n",
    "        ])\n",
    "\n",
    "    def _load_document_mappings(self) -> Dict[str, str]:\n",
    "        \"\"\"Load document name to path mappings from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.documents_json_path, 'r', encoding='utf-8') as f:\n",
    "                mappings = json.load(f)\n",
    "            logger.info(f\"Loaded {len(mappings)} document mappings\")\n",
    "            return mappings\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Document mappings file not found: {self.documents_json_path}\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error parsing document mappings JSON: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _format_available_documents(self) -> str:\n",
    "        \"\"\"Format available documents for the prompt\"\"\"\n",
    "        if not self.document_mappings:\n",
    "            return \"No documents currently available.\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for doc_name in self.document_mappings.keys():\n",
    "            doc_list.append(f\"- {doc_name}\")\n",
    "        \n",
    "        return \"\\n\".join(doc_list)\n",
    "\n",
    "    def _find_document(self, document_name: str, search_terms: Optional[List[str]] = None) -> Optional[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Find a document by name or search terms\n",
    "        \n",
    "        Args:\n",
    "            document_name: The requested document name\n",
    "            search_terms: Alternative search terms\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (matched_name, file_path) or None if not found\n",
    "        \"\"\"\n",
    "        # Direct name match (case-insensitive)\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            if doc_name.lower() == document_name.lower():\n",
    "                if os.path.exists(doc_path):\n",
    "                    return doc_name, doc_path\n",
    "                else:\n",
    "                    logger.warning(f\"Document found in mapping but file doesn't exist: {doc_path}\")\n",
    "        \n",
    "        # Partial name match\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            if document_name.lower() in doc_name.lower() or doc_name.lower() in document_name.lower():\n",
    "                if os.path.exists(doc_path):\n",
    "                    return doc_name, doc_path\n",
    "        \n",
    "        # Search terms match\n",
    "        if search_terms:\n",
    "            for term in search_terms:\n",
    "                for doc_name, doc_path in self.document_mappings.items():\n",
    "                    if term.lower() in doc_name.lower():\n",
    "                        if os.path.exists(doc_path):\n",
    "                            return doc_name, doc_path\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def retrieve_document(self, document_name: str, search_terms: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Retrieve a document by name\n",
    "        \n",
    "        Args:\n",
    "            document_name: Name of the document to retrieve\n",
    "            search_terms: Alternative search terms\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with document info and retrieval status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self._find_document(document_name, search_terms)\n",
    "            \n",
    "            if result is None:\n",
    "                available_docs = list(self.document_mappings.keys())\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"message\": f\"Document '{document_name}' not found.\",\n",
    "                    \"available_documents\": available_docs,\n",
    "                    \"suggestion\": f\"Available documents: {', '.join(available_docs[:5])}{'...' if len(available_docs) > 5 else ''}\"\n",
    "                }\n",
    "            \n",
    "            matched_name, file_path = result\n",
    "            \n",
    "            # Get file info\n",
    "            file_stat = os.stat(file_path)\n",
    "            file_size = file_stat.st_size\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            # Get MIME type\n",
    "            mime_type, _ = mimetypes.guess_type(file_path)\n",
    "            \n",
    "            logger.info(f\"Successfully retrieved document: {matched_name} ({file_size_mb:.2f} MB)\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"document_name\": matched_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"file_size\": file_size,\n",
    "                \"file_size_mb\": round(file_size_mb, 2),\n",
    "                \"mime_type\": mime_type or \"application/pdf\",\n",
    "                \"message\": f\"Document '{matched_name}' is ready for download.\",\n",
    "                \"download_info\": {\n",
    "                    \"filename\": os.path.basename(file_path),\n",
    "                    \"extension\": Path(file_path).suffix\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving document '{document_name}': {str(e)}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": f\"Error retrieving document: {str(e)}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def process_user_query(self, user_query: str) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a user query and determine if they're asking for a document\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's request\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (response_message, metadata)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create the chain with function calling\n",
    "            chain = self.decision_prompt | self.llm.bind(\n",
    "                functions=[self.function_definition],\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "            \n",
    "            metadata = {\n",
    "                \"user_query\": user_query,\n",
    "                \"document_requested\": False,\n",
    "                \"document_retrieved\": False,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"available_documents_count\": len(self.document_mappings)\n",
    "            }\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain.invoke({\"user_query\": user_query})\n",
    "            \n",
    "            # Track decision-making cost\n",
    "            metadata[\"decision_cost\"] = {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "            metadata[\"total_cost\"] += cb.total_cost\n",
    "            \n",
    "            # Check if the model decided to use function calling\n",
    "            if hasattr(response, 'additional_kwargs') and 'function_call' in response.additional_kwargs:\n",
    "                function_call = response.additional_kwargs['function_call']\n",
    "                function_name = function_call['name']\n",
    "                function_args = json.loads(function_call['arguments'])\n",
    "                \n",
    "                logger.info(f\"LLM decided to retrieve document: {function_args}\")\n",
    "                metadata[\"document_requested\"] = True\n",
    "                metadata[\"function_args\"] = function_args\n",
    "                \n",
    "                if function_name == \"retrieve_document\":\n",
    "                    # Execute document retrieval\n",
    "                    retrieval_result = self.retrieve_document(\n",
    "                        document_name=function_args.get('document_name', ''),\n",
    "                        search_terms=function_args.get('search_terms', [])\n",
    "                    )\n",
    "                    \n",
    "                    metadata.update(retrieval_result)\n",
    "                    metadata[\"document_retrieved\"] = retrieval_result.get(\"success\", False)\n",
    "                    \n",
    "                    if retrieval_result.get(\"success\"):\n",
    "                        doc_info = retrieval_result\n",
    "                        response_message = (\n",
    "                            f\"✅ **{doc_info['document_name']}** is ready for download!\\n\\n\"\n",
    "                            f\"📄 **File:** {doc_info['download_info']['filename']}\\n\"\n",
    "                            f\"📊 **Size:** {doc_info['file_size_mb']} MB\\n\"\n",
    "                            f\"📁 **Location:** {doc_info['file_path']}\\n\\n\"\n",
    "                            f\"You can download the document from the specified location.\"\n",
    "                        )\n",
    "                        return response_message, metadata\n",
    "                    else:\n",
    "                        response_message = (\n",
    "                            f\"❌ {retrieval_result['message']}\\n\\n\"\n",
    "                            f\"{retrieval_result.get('suggestion', '')}\"\n",
    "                        )\n",
    "                        return response_message, metadata\n",
    "                        \n",
    "            else:\n",
    "                # LLM decided user is not asking for a document\n",
    "                response_message = (\n",
    "                    \"I don't see a request for a specific document in your message. \"\n",
    "                    f\"If you're looking for a document, I have access to {len(self.document_mappings)} documents. \"\n",
    "                    \"You can ask me to retrieve documents like 'Can I get the user manual?' or 'I need the API documentation'.\"\n",
    "                )\n",
    "                return response_message, metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_user_query: {str(e)}\")\n",
    "            metadata[\"error\"] = str(e)\n",
    "            return f\"I encountered an error while processing your request: {str(e)}\", metadata\n",
    "\n",
    "    def list_available_documents(self) -> str:\n",
    "        \"\"\"Return a formatted list of available documents\"\"\"\n",
    "        if not self.document_mappings:\n",
    "            return \"No documents are currently available.\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for i, (doc_name, doc_path) in enumerate(self.document_mappings.items(), 1):\n",
    "            file_exists = \"✅\" if os.path.exists(doc_path) else \"❌\"\n",
    "            doc_list.append(f\"{i}. {file_exists} **{doc_name}**\")\n",
    "        \n",
    "        return f\"**Available Documents ({len(self.document_mappings)}):**\\n\\n\" + \"\\n\".join(doc_list)\n",
    "\n",
    "    def reload_document_mappings(self) -> bool:\n",
    "        \"\"\"Reload document mappings from the JSON file\"\"\"\n",
    "        try:\n",
    "            self.document_mappings = self._load_document_mappings()\n",
    "            logger.info(\"Document mappings reloaded successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to reload document mappings: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "# Enhanced Smart RAG Tool that includes PDF retrieval\n",
    "class EnhancedSmartRAGTool:\n",
    "    def __init__(self, \n",
    "                 local_index_path: str,\n",
    "                 embedding_model,\n",
    "                 documents_json_path: str,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Enhanced Smart RAG Tool with PDF retrieval capability\n",
    "        \"\"\"\n",
    "        from .smart_rag_tool import SmartRAGTool  # Import your existing Smart RAG Tool\n",
    "        \n",
    "        self.smart_rag = SmartRAGTool(local_index_path, embedding_model, llm_params)\n",
    "        self.pdf_tool = PDFRetrievalTool(documents_json_path, llm_params)\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "\n",
    "    def process_query(self, user_query: str, chat_context: Optional[str] = None, verbose: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Process a user query with both document retrieval and RAG capabilities\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question or request\n",
    "            chat_context: Optional conversation context\n",
    "            verbose: Whether to print detailed information\n",
    "            \n",
    "        Returns:\n",
    "            The response string\n",
    "        \"\"\"\n",
    "        # First, check if user is asking for a document\n",
    "        doc_response, doc_metadata = self.pdf_tool.process_user_query(user_query)\n",
    "        \n",
    "        if doc_metadata.get(\"document_requested\", False):\n",
    "            if verbose:\n",
    "                print(f\"\\n--- Document Retrieval Attempt ---\")\n",
    "                print(f\"Document Requested: {doc_metadata.get('document_requested', False)}\")\n",
    "                print(f\"Document Retrieved: {doc_metadata.get('document_retrieved', False)}\")\n",
    "                print(f\"Cost: ${doc_metadata.get('total_cost', 0):.4f}\")\n",
    "                print(f\"--- End Document Retrieval ---\\n\")\n",
    "            \n",
    "            return doc_response\n",
    "        \n",
    "        # If not asking for a document, use Smart RAG\n",
    "        return self.smart_rag.chat(user_query, chat_context, verbose)\n",
    "\n",
    "    def list_documents(self) -> str:\n",
    "        \"\"\"List all available documents\"\"\"\n",
    "        return self.pdf_tool.list_available_documents()\n",
    "\n",
    "    def reload_documents(self) -> bool:\n",
    "        \"\"\"Reload document mappings\"\"\"\n",
    "        return self.pdf_tool.reload_document_mappings()\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "def create_enhanced_rag_tool(local_index_path: str, \n",
    "                           embedding_model, \n",
    "                           documents_json_path: str) -> EnhancedSmartRAGTool:\n",
    "    \"\"\"Factory function to create an Enhanced Smart RAG Tool instance\"\"\"\n",
    "    return EnhancedSmartRAGTool(\n",
    "        local_index_path=local_index_path,\n",
    "        embedding_model=embedding_model,\n",
    "        documents_json_path=documents_json_path,\n",
    "        llm_params={\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example JSON structure for documents.json:\n",
    "    # {\n",
    "    #     \"User Manual\": \"/path/to/user_manual.pdf\",\n",
    "    #     \"API Documentation\": \"/path/to/api_docs.pdf\",\n",
    "    #     \"Installation Guide\": \"/path/to/install_guide.pdf\",\n",
    "    #     \"Quarterly Report Q1 2024\": \"/path/to/q1_report.pdf\"\n",
    "    # }\n",
    "    \n",
    "    # Initialize the enhanced tool\n",
    "    # enhanced_rag = create_enhanced_rag_tool(\n",
    "    #     local_index_path=\"./faiss_index\",\n",
    "    #     embedding_model=your_embedding_model,\n",
    "    #     documents_json_path=\"./documents.json\"\n",
    "    # )\n",
    "    \n",
    "    # Example interactions:\n",
    "    # enhanced_rag.process_query(\"Can I get the user manual?\", verbose=True)  # Document retrieval\n",
    "    # enhanced_rag.process_query(\"How does authentication work?\", verbose=True)  # RAG search\n",
    "    # enhanced_rag.process_query(\"What is machine learning?\", verbose=True)  # Direct answer\n",
    "    # print(enhanced_rag.list_documents())  # List available documents\n",
    "    \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-rag-83-kaUDo3eQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
