{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ade1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import your existing RAG fusion function\n",
    "from rag_pipeline.rag_fusion_pipeline import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SmartRAGTool:\n",
    "    def __init__(self, \n",
    "                 local_index_path: str,\n",
    "                 embedding_model,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Smart RAG Tool\n",
    "        \n",
    "        Args:\n",
    "            local_index_path: Path to the FAISS vector store\n",
    "            embedding_model: Embedding model for vector store\n",
    "            llm_params: Parameters for the LLM\n",
    "        \"\"\"\n",
    "        self.local_index_path = local_index_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "        self.llm = ChatOpenAI(**self.llm_params)\n",
    "        \n",
    "        # Function definition for OpenAI function calling\n",
    "        self.function_definition = {\n",
    "            \"name\": \"search_knowledge_base\",\n",
    "            \"description\": \"Search the knowledge base for specific information when the question requires domain-specific or detailed factual information that may not be in general knowledge\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to find relevant information\"\n",
    "                    },\n",
    "                    \"mode\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"original\", \"generated\"],\n",
    "                        \"description\": \"Search mode: 'original' uses only the user query, 'generated' creates multiple related queries for better coverage\"\n",
    "                    },\n",
    "                    \"num_queries\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"minimum\": 1,\n",
    "                        \"maximum\": 10,\n",
    "                        \"description\": \"Number of queries to generate if using 'generated' mode (default: 3)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Prompt to determine if RAG is needed\n",
    "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an AI assistant that decides whether a user question requires searching a knowledge base or can be answered with general knowledge.\n",
    "\n",
    "Use the search_knowledge_base function ONLY when:\n",
    "1. The questions is specific and realted to a policy, fact, legal specification.\n",
    "2. The question requires current or specific information that might not be in general knowledge\n",
    "\n",
    "DO NOT use the search function for:\n",
    "1. General knowledge questions (e.g., \"What is machine learning?\", \"How does photosynthesis work?\")\n",
    "2. Questions not related to the knowledge questions should be politely redirected to ask user for questions that are related to the knowledge base.\n",
    "\n",
    "If you decide to search, choose the appropriate mode:\n",
    "- Use \"original\" mode for simple, direct queries\n",
    "- Use \"generated\" mode for complex questions that might benefit from multiple search perspectives\n",
    "\n",
    "If you don't need to search, answer the question directly using your general knowledge.\"\"\"),\n",
    "            (\"user\", \"{user_query}\")\n",
    "        ])\n",
    "\n",
    "    def search_knowledge_base(self, query: str, mode: str = \"generated\", num_queries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Search the knowledge base using RAG fusion\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            mode: Search mode ('original' or 'generated')\n",
    "            num_queries: Number of queries to generate if using 'generated' mode\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Searching knowledge base with query: '{query}' in {mode} mode\")\n",
    "            \n",
    "            answer, metadata = rag_fusion_answer(\n",
    "                user_query=query,\n",
    "                local_index_path=self.local_index_path,\n",
    "                embedding_model=self.embedding_model,\n",
    "                mode=mode,\n",
    "                num_generated_queries=num_queries,\n",
    "                top_k=5,  # Retrieve more documents for better context\n",
    "                params=self.llm_params\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"metadata\": metadata,\n",
    "                \"search_performed\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in knowledge base search: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": f\"I encountered an error while searching the knowledge base: {str(e)}\",\n",
    "                \"metadata\": {},\n",
    "                \"search_performed\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def process_user_query(self, user_query: str, chat_context: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a user query and decide whether to use RAG or answer directly\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            chat_context: Optional conversation context\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (answer, metadata)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create the chain with function calling\n",
    "            chain = self.decision_prompt | self.llm.bind(\n",
    "                functions=[self.function_definition],\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "            \n",
    "            metadata = {\n",
    "                \"user_query\": user_query,\n",
    "                \"decision_made\": None,\n",
    "                \"search_performed\": False,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"token_usage\": {}\n",
    "            }\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain.invoke({\"user_query\": user_query})\n",
    "            \n",
    "            # Track decision-making cost\n",
    "            decision_cost = {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "            metadata[\"decision_cost\"] = decision_cost\n",
    "            metadata[\"total_cost\"] += cb.total_cost\n",
    "            \n",
    "            # Check if the model decided to use function calling\n",
    "            if hasattr(response, 'additional_kwargs') and 'function_call' in response.additional_kwargs:\n",
    "                function_call = response.additional_kwargs['function_call']\n",
    "                function_name = function_call['name']\n",
    "                function_args = json.loads(function_call['arguments'])\n",
    "                \n",
    "                logger.info(f\"LLM decided to use function: {function_name} with args: {function_args}\")\n",
    "                metadata[\"decision_made\"] = \"search_needed\"\n",
    "                \n",
    "                if function_name == \"search_knowledge_base\":\n",
    "                    # Execute the RAG search\n",
    "                    search_result = self.search_knowledge_base(\n",
    "                        query=function_args.get('query', user_query),\n",
    "                        mode=function_args.get('mode', 'generated'),\n",
    "                        num_queries=function_args.get('num_queries', 3)\n",
    "                    )\n",
    "                    \n",
    "                    if search_result.get(\"search_performed\"):\n",
    "                        metadata.update(search_result[\"metadata\"])\n",
    "                        metadata[\"search_performed\"] = True\n",
    "                        metadata[\"total_cost\"] += search_result[\"metadata\"].get(\"total_price\", 0)\n",
    "                        \n",
    "                        return search_result[\"answer\"], metadata\n",
    "                    else:\n",
    "                        # Fallback if search failed\n",
    "                        return f\"I tried to search for information but encountered an issue. Based on general knowledge: I'd be happy to help, but I may need more specific information to give you the most accurate answer.\", metadata\n",
    "                        \n",
    "            else:\n",
    "                # LLM decided not to search - use the direct response\n",
    "                logger.info(\"LLM decided no search needed, providing direct answer\")\n",
    "                metadata[\"decision_made\"] = \"direct_answer\"\n",
    "                return response.content, metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_user_query: {str(e)}\")\n",
    "            metadata[\"error\"] = str(e)\n",
    "            return f\"I encountered an error while processing your question: {str(e)}\", metadata\n",
    "\n",
    "    def chat(self, user_query: str, chat_context: Optional[str] = None, verbose: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Simple chat interface that handles the query and returns just the answer\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            chat_context: Optional conversation context\n",
    "            verbose: Whether to print detailed metadata\n",
    "            \n",
    "        Returns:\n",
    "            The answer string\n",
    "        \"\"\"\n",
    "        answer, metadata = self.process_user_query(user_query, chat_context)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Smart RAG Tool Execution Report ---\")\n",
    "            print(f\"User Query: {user_query}\")\n",
    "            print(f\"Decision Made: {metadata.get('decision_made', 'unknown')}\")\n",
    "            print(f\"Search Performed: {metadata.get('search_performed', False)}\")\n",
    "            print(f\"Total Cost: ${metadata.get('total_cost', 0):.4f}\")\n",
    "            \n",
    "            if metadata.get('search_performed'):\n",
    "                token_usage = metadata.get('token_usage', {})\n",
    "                print(f\"Total Tokens Used: {token_usage.get('total_tokens', 0)}\")\n",
    "                print(f\"Queries Used: {metadata.get('queries_used', [])}\")\n",
    "                print(f\"Documents Retrieved: {metadata.get('num_documents_retrieved', 0)}\")\n",
    "            \n",
    "            print(f\"--- End Report ---\\n\")\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "def create_smart_rag_tool(local_index_path: str, embedding_model) -> SmartRAGTool:\n",
    "    \"\"\"Factory function to create a Smart RAG Tool instance\"\"\"\n",
    "    return SmartRAGTool(\n",
    "        local_index_path=local_index_path,\n",
    "        embedding_model=embedding_model,\n",
    "        llm_params={\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "    # Initialize the tool\n",
    "    # smart_rag = create_smart_rag_tool(\"./faiss_index\", your_embedding_model)\n",
    "    \n",
    "    # Example questions that would trigger different behaviors:\n",
    "    \n",
    "    # This would likely NOT trigger RAG (general knowledge)\n",
    "    # answer = smart_rag.chat(\"What is machine learning?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # This would likely trigger RAG (specific/domain knowledge)\n",
    "    # answer = smart_rag.chat(\"What are the specific implementation details of our authentication system?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # This would likely trigger RAG with generated mode (complex query)\n",
    "    # answer = smart_rag.chat(\"How does our system handle user permissions and what are the security implications?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7274cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "smart_rag = create_smart_rag_tool(\"./data/faiss_index\", OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided to use function: search_knowledge_base with args: {'query': 'пороговый уровень ОЗП', 'mode': 'original'}\n",
      "INFO:__main__:Searching knowledge base with query: 'пороговый уровень ОЗП' in original mode\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "INFO:rag_pipeline.rag_fusion_pipeline:Running in original query mode.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart RAG Tool Execution Report ---\n",
      "User Query: Какой пороговый уровень ОЗП\n",
      "Decision Made: search_needed\n",
      "Search Performed: True\n",
      "Total Cost: $0.0065\n",
      "Total Tokens Used: 1720\n",
      "Queries Used: ['пороговый уровень ОЗП']\n",
      "Documents Retrieved: 4\n",
      "--- End Report ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Пороговый уровень для оценки знаний педагогов (ОЗП) зависит от квалификационной категории педагога. Согласно предоставленным документам, пороговые уровни следующие:\\n\\n- Для квалификационной категории «педагог-стажер/педагог» - 50%;\\n- Для квалификационной категории «педагог-модератор» - 60%;\\n- Для квалификационной категории «педагог-эксперт» - 70%;\\n- Для квалификационной категории «педагог-исследователь» - 80%;\\n- Для квалификационной категории «педагог-мастер» - 90%.\\n\\nДля первых руководителей, заместителей руководителя организаций образования и методических кабинетов (центров) пороговый уровень составляет 70%.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = smart_rag.chat(\"Какой пороговый уровень ОЗП\", verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe0a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided no search needed, providing direct answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart RAG Tool Execution Report ---\n",
      "User Query: Какая столица Франции?\n",
      "Decision Made: direct_answer\n",
      "Search Performed: False\n",
      "Total Cost: $0.0009\n",
      "--- End Report ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Столица Франции — Париж.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_rag.chat(\"Какая столица Франции?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9e8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFRetrievalTool:\n",
    "    def __init__(self, \n",
    "                 documents_json_path: str,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the PDF Retrieval Tool\n",
    "        \n",
    "        Args:\n",
    "            documents_json_path: Path to JSON file containing document mappings\n",
    "            llm_params: Parameters for the LLM\n",
    "        \"\"\"\n",
    "        self.documents_json_path = documents_json_path\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "        self.llm = ChatOpenAI(**self.llm_params)\n",
    "        \n",
    "        # Load document mappings\n",
    "        self.document_mappings = self._load_document_mappings()\n",
    "        \n",
    "        # Create enhanced search indices\n",
    "        self.search_index = self._create_search_index()\n",
    "        \n",
    "        # Enhanced function definition for OpenAI function calling\n",
    "        self.function_definition = {\n",
    "            \"name\": \"retrieve_document\",\n",
    "            \"description\": \"Retrieve a specific document when user asks for any document, file, manual, guide, report, or wants to download/access/get any document\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"document_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name or identifier of the document to retrieve\"\n",
    "                    },\n",
    "                    \"search_keywords\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Keywords extracted from user query to help find the document\"\n",
    "                    },\n",
    "                    \"document_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Type of document (manual, guide, report, documentation, etc.)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"document_name\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # More aggressive prompt to catch document requests\n",
    "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are a document retrieval assistant. Your job is to identify when users want to access, download, or retrieve ANY document.\n",
    "\n",
    "AVAILABLE DOCUMENTS:\n",
    "{self._format_available_documents_detailed()}\n",
    "\n",
    "ALWAYS use the retrieve_document function when users:\n",
    "- Ask for ANY document, file, manual, guide, report, or PDF\n",
    "- Want to \"download\", \"get\", \"retrieve\", \"access\", \"show\", \"open\", or \"see\" a document\n",
    "- Ask questions like \"do you have...\", \"can I get...\", \"where is...\", \"find...\" followed by document-related terms\n",
    "- Mention specific document types (manual, guide, documentation, report, etc.)\n",
    "- Use phrases like \"I need the...\", \"show me the...\", \"give me the...\"\n",
    "\n",
    "EXAMPLES that should trigger retrieve_document:\n",
    "✅ \"Can I get the user manual?\"\n",
    "✅ \"I need the API documentation\"  \n",
    "✅ \"Do you have the installation guide?\"\n",
    "✅ \"Show me the quarterly report\"\n",
    "✅ \"Where is the policy document?\"\n",
    "✅ \"Find the technical specifications\"\n",
    "✅ \"I want to see the handbook\"\n",
    "✅ \"Download the reference guide\"\n",
    "✅ \"Give me the troubleshooting manual\"\n",
    "✅ \"Can you provide the system documentation?\"\n",
    "\n",
    "When calling retrieve_document:\n",
    "1. Extract the main document name/type from the user's request\n",
    "2. Include relevant keywords from their query in search_keywords\n",
    "3. Specify the document_type if identifiable\n",
    "\n",
    "Be very liberal in detecting document requests - when in doubt, assume they want a document.\"\"\"),\n",
    "            (\"user\", \"{user_query}\")\n",
    "        ])\n",
    "\n",
    "    def _load_document_mappings(self) -> Dict[str, str]:\n",
    "        \"\"\"Load document name to path mappings from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.documents_json_path, 'r', encoding='utf-8') as f:\n",
    "                mappings = json.load(f)\n",
    "            logger.info(f\"Loaded {len(mappings)} document mappings\")\n",
    "            return mappings\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Document mappings file not found: {self.documents_json_path}\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error parsing document mappings JSON: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _create_search_index(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Create enhanced search index with keywords for each document\"\"\"\n",
    "        search_index = {}\n",
    "        \n",
    "        for doc_name in self.document_mappings.keys():\n",
    "            # Extract keywords from document name\n",
    "            keywords = self._extract_keywords(doc_name)\n",
    "            search_index[doc_name] = keywords\n",
    "            \n",
    "        return search_index\n",
    "\n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract searchable keywords from text\"\"\"\n",
    "        # Convert to lowercase and split on common separators\n",
    "        text = text.lower()\n",
    "        # Split on various separators and remove empty strings\n",
    "        words = re.split(r'[_\\-\\s\\.\\(\\)\\[\\]]+', text)\n",
    "        words = [w.strip() for w in words if w.strip()]\n",
    "        \n",
    "        # Add common synonyms and variations\n",
    "        expanded_words = set(words)\n",
    "        \n",
    "        # Add synonyms for common terms\n",
    "        synonyms = {\n",
    "            'manual': ['guide', 'handbook', 'documentation', 'doc', 'instructions'],\n",
    "            'guide': ['manual', 'handbook', 'documentation', 'tutorial', 'howto'],\n",
    "            'api': ['interface', 'endpoint', 'service', 'programming'],\n",
    "            'install': ['installation', 'setup', 'deployment', 'configure'],\n",
    "            'user': ['users', 'customer', 'client'],\n",
    "            'admin': ['administrator', 'administration', 'management'],\n",
    "            'tech': ['technical', 'technology'],\n",
    "            'spec': ['specification', 'specifications', 'specs'],\n",
    "            'ref': ['reference', 'references'],\n",
    "            'trouble': ['troubleshooting', 'troubleshoot', 'debug', 'problem'],\n",
    "        }\n",
    "        \n",
    "        for word in words:\n",
    "            if word in synonyms:\n",
    "                expanded_words.update(synonyms[word])\n",
    "        \n",
    "        return list(expanded_words)\n",
    "\n",
    "    def _format_available_documents_detailed(self) -> str:\n",
    "        \"\"\"Format available documents with more detail for better matching\"\"\"\n",
    "        if not self.document_mappings:\n",
    "            return \"No documents currently available.\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            keywords = ', '.join(self.search_index.get(doc_name, [])[:5])  # Show first 5 keywords\n",
    "            status = \"✅\" if os.path.exists(doc_path) else \"❌\"\n",
    "            doc_list.append(f\"{status} **{doc_name}**\\n   Keywords: {keywords}\")\n",
    "        \n",
    "        return \"\\n\".join(doc_list)\n",
    "\n",
    "    def _calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate similarity between two strings\"\"\"\n",
    "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "    def _find_document_enhanced(self, document_name: str, search_keywords: Optional[List[str]] = None, document_type: Optional[str] = None) -> Optional[Tuple[str, str, float]]:\n",
    "        \"\"\"\n",
    "        Enhanced document finding with multiple strategies\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (matched_name, file_path, confidence_score) or None\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        # Strategy 1: Exact name match\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            if doc_name.lower() == document_name.lower():\n",
    "                if os.path.exists(doc_path):\n",
    "                    return doc_name, doc_path, 1.0\n",
    "        \n",
    "        # Strategy 2: High similarity match\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            similarity = self._calculate_similarity(document_name, doc_name)\n",
    "            if similarity > 0.8 and os.path.exists(doc_path):\n",
    "                candidates.append((doc_name, doc_path, similarity))\n",
    "        \n",
    "        # Strategy 3: Partial name matching\n",
    "        document_name_lower = document_name.lower()\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            doc_name_lower = doc_name.lower()\n",
    "            if (document_name_lower in doc_name_lower or \n",
    "                doc_name_lower in document_name_lower or\n",
    "                any(word in doc_name_lower for word in document_name_lower.split() if len(word) > 2)):\n",
    "                if os.path.exists(doc_path):\n",
    "                    similarity = self._calculate_similarity(document_name, doc_name)\n",
    "                    candidates.append((doc_name, doc_path, similarity))\n",
    "        \n",
    "        # Strategy 4: Keyword matching\n",
    "        if search_keywords:\n",
    "            for doc_name, doc_path in self.document_mappings.items():\n",
    "                doc_keywords = self.search_index.get(doc_name, [])\n",
    "                keyword_matches = 0\n",
    "                for keyword in search_keywords:\n",
    "                    keyword_lower = keyword.lower()\n",
    "                    for doc_keyword in doc_keywords:\n",
    "                        if (keyword_lower == doc_keyword or \n",
    "                            keyword_lower in doc_keyword or \n",
    "                            doc_keyword in keyword_lower):\n",
    "                            keyword_matches += 1\n",
    "                            break\n",
    "                \n",
    "                if keyword_matches > 0 and os.path.exists(doc_path):\n",
    "                    # Calculate confidence based on keyword matches\n",
    "                    confidence = min(0.9, keyword_matches / len(search_keywords) * 0.8)\n",
    "                    candidates.append((doc_name, doc_path, confidence))\n",
    "        \n",
    "        # Strategy 5: Document type matching\n",
    "        if document_type:\n",
    "            document_type_lower = document_type.lower()\n",
    "            for doc_name, doc_path in self.document_mappings.items():\n",
    "                if document_type_lower in doc_name.lower() and os.path.exists(doc_path):\n",
    "                    similarity = self._calculate_similarity(document_type, doc_name)\n",
    "                    candidates.append((doc_name, doc_path, similarity * 0.7))\n",
    "        \n",
    "        # Remove duplicates and sort by confidence\n",
    "        unique_candidates = {}\n",
    "        for name, path, confidence in candidates:\n",
    "            if name not in unique_candidates or confidence > unique_candidates[name][2]:\n",
    "                unique_candidates[name] = (name, path, confidence)\n",
    "        \n",
    "        if unique_candidates:\n",
    "            # Return the best match\n",
    "            best_match = max(unique_candidates.values(), key=lambda x: x[2])\n",
    "            return best_match\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def retrieve_document(self, document_name: str, search_keywords: Optional[List[str]] = None, document_type: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced document retrieval with better matching\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self._find_document_enhanced(document_name, search_keywords, document_type)\n",
    "            \n",
    "            if result is None:\n",
    "                # Provide better suggestions\n",
    "                suggestions = self._get_suggestions(document_name, search_keywords, document_type)\n",
    "                available_docs = list(self.document_mappings.keys())\n",
    "                \n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"message\": f\"Document '{document_name}' not found.\",\n",
    "                    \"available_documents\": available_docs,\n",
    "                    \"suggestions\": suggestions,\n",
    "                    \"suggestion_text\": f\"Did you mean: {', '.join(suggestions[:3])}?\" if suggestions else \"Please check available documents.\"\n",
    "                }\n",
    "            \n",
    "            matched_name, file_path, confidence = result\n",
    "            \n",
    "            # Get file info\n",
    "            file_stat = os.stat(file_path)\n",
    "            file_size = file_stat.st_size\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            # Get MIME type\n",
    "            mime_type, _ = mimetypes.guess_type(file_path)\n",
    "            \n",
    "            logger.info(f\"Successfully retrieved document: {matched_name} (confidence: {confidence:.2f}, size: {file_size_mb:.2f} MB)\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"document_name\": matched_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"file_size\": file_size,\n",
    "                \"file_size_mb\": round(file_size_mb, 2),\n",
    "                \"mime_type\": mime_type or \"application/pdf\",\n",
    "                \"confidence\": round(confidence, 2),\n",
    "                \"message\": f\"Document '{matched_name}' is ready for download (confidence: {confidence:.1%}).\",\n",
    "                \"download_info\": {\n",
    "                    \"filename\": os.path.basename(file_path),\n",
    "                    \"extension\": Path(file_path).suffix\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving document '{document_name}': {str(e)}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": f\"Error retrieving document: {str(e)}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def _get_suggestions(self, document_name: str, search_keywords: Optional[List[str]] = None, document_type: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Get document suggestions based on partial matches\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        # Find partial matches\n",
    "        for doc_name in self.document_mappings.keys():\n",
    "            similarity = self._calculate_similarity(document_name, doc_name)\n",
    "            if similarity > 0.3:  # Lower threshold for suggestions\n",
    "                suggestions.append((doc_name, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top suggestions\n",
    "        suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [name for name, _ in suggestions[:5]]\n",
    "\n",
    "    def process_user_query(self, user_query: str) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Enhanced query processing with better document detection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Force function calling to be more aggressive\n",
    "            chain = self.decision_prompt | self.llm.bind(\n",
    "                functions=[self.function_definition],\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "            \n",
    "            metadata = {\n",
    "                \"user_query\": user_query,\n",
    "                \"document_requested\": False,\n",
    "                \"document_retrieved\": False,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"available_documents_count\": len(self.document_mappings)\n",
    "            }\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain.invoke({\"user_query\": user_query})\n",
    "            \n",
    "            # Track costs\n",
    "            metadata[\"decision_cost\"] = {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "            metadata[\"total_cost\"] += cb.total_cost\n",
    "            \n",
    "            # Check for function call in response\n",
    "            function_call = None\n",
    "            if hasattr(response, 'additional_kwargs') and 'function_call' in response.additional_kwargs:\n",
    "                function_call = response.additional_kwargs['function_call']\n",
    "            elif hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "                # Handle newer OpenAI API format\n",
    "                function_call = response.tool_calls[0].function if response.tool_calls else None\n",
    "            \n",
    "            if function_call:\n",
    "                try:\n",
    "                    function_name = function_call.name if hasattr(function_call, 'name') else function_call['name']\n",
    "                    function_args = json.loads(function_call.arguments if hasattr(function_call, 'arguments') else function_call['arguments'])\n",
    "                    \n",
    "                    logger.info(f\"LLM decided to retrieve document: {function_args}\")\n",
    "                    metadata[\"document_requested\"] = True\n",
    "                    metadata[\"function_args\"] = function_args\n",
    "                    \n",
    "                    if function_name == \"retrieve_document\":\n",
    "                        # Execute enhanced document retrieval\n",
    "                        retrieval_result = self.retrieve_document(\n",
    "                            document_name=function_args.get('document_name', ''),\n",
    "                            search_keywords=function_args.get('search_keywords', []),\n",
    "                            document_type=function_args.get('document_type', '')\n",
    "                        )\n",
    "                        \n",
    "                        metadata.update(retrieval_result)\n",
    "                        metadata[\"document_retrieved\"] = retrieval_result.get(\"success\", False)\n",
    "                        \n",
    "                        if retrieval_result.get(\"success\"):\n",
    "                            doc_info = retrieval_result\n",
    "                            response_message = (\n",
    "                                f\"✅ **{doc_info['document_name']}** found and ready for download!\\n\\n\"\n",
    "                                f\"📄 **File:** {doc_info['download_info']['filename']}\\n\"\n",
    "                                f\"📊 **Size:** {doc_info['file_size_mb']} MB\\n\"\n",
    "                                f\"🎯 **Confidence:** {doc_info['confidence']:.1%}\\n\"\n",
    "                                f\"📁 **Location:** {doc_info['file_path']}\\n\\n\"\n",
    "                                f\"The document is ready for download from the specified location.\"\n",
    "                            )\n",
    "                        else:\n",
    "                            response_message = (\n",
    "                                f\"❌ {retrieval_result['message']}\\n\\n\"\n",
    "                                f\"💡 **{retrieval_result.get('suggestion_text', '')}**\\n\\n\"\n",
    "                                f\"📋 **Available documents:** {', '.join(list(self.document_mappings.keys())[:3])}...\"\n",
    "                            )\n",
    "                        \n",
    "                        return response_message, metadata\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing function call: {e}\")\n",
    "                    metadata[\"error\"] = str(e)\n",
    "            \n",
    "            # If no function call, provide helpful response\n",
    "            response_message = (\n",
    "                f\"I didn't detect a specific document request in your message. \"\n",
    "                f\"I have access to {len(self.document_mappings)} documents. \"\n",
    "                f\"Try asking something like:\\n\"\n",
    "                f\"• 'Can I get the [document name]?'\\n\"\n",
    "                f\"• 'I need the [type] documentation'\\n\"\n",
    "                f\"• 'Show me the [document] guide'\\n\\n\"\n",
    "                f\"Available documents: {', '.join(list(self.document_mappings.keys())[:3])}...\"\n",
    "            )\n",
    "            return response_message, metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_user_query: {str(e)}\")\n",
    "            metadata[\"error\"] = str(e)\n",
    "            return f\"I encountered an error while processing your request: {str(e)}\", metadata\n",
    "\n",
    "    def list_available_documents(self) -> str:\n",
    "        \"\"\"Return a formatted list of available documents with enhanced info\"\"\"\n",
    "        if not self.document_mappings:\n",
    "            return \"No documents are currently available.\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for i, (doc_name, doc_path) in enumerate(self.document_mappings.items(), 1):\n",
    "            file_exists = \"✅\" if os.path.exists(doc_path) else \"❌\"\n",
    "            \n",
    "            # Get file size if exists\n",
    "            size_info = \"\"\n",
    "            if os.path.exists(doc_path):\n",
    "                try:\n",
    "                    size_mb = os.path.getsize(doc_path) / (1024 * 1024)\n",
    "                    size_info = f\" ({size_mb:.1f} MB)\"\n",
    "                except:\n",
    "                    size_info = \"\"\n",
    "            \n",
    "            # Get keywords\n",
    "            keywords = ', '.join(self.search_index.get(doc_name, [])[:3])\n",
    "            \n",
    "            doc_list.append(\n",
    "                f\"{i}. {file_exists} **{doc_name}**{size_info}\\n\"\n",
    "                f\"   🏷️ Keywords: {keywords}\"\n",
    "            )\n",
    "        \n",
    "        return f\"**Available Documents ({len(self.document_mappings)}):**\\n\\n\" + \"\\n\\n\".join(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811838af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 16 document mappings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF Retrieval Tool initialized\n"
     ]
    }
   ],
   "source": [
    "def setup_pdf_tool():\n",
    "    \"\"\"Set up the PDF retrieval tool\"\"\"\n",
    "    \n",
    "    # Create documents.json if it doesn't exist\n",
    "    documents_json_path = \"documents.json\"\n",
    "    if not os.path.exists(documents_json_path):\n",
    "        documents_json_path = create_documents_json()\n",
    "    \n",
    "    # Initialize the too\n",
    "    \n",
    "    # Configure LLM parameters (make sure you have OPENAI_API_KEY set)\n",
    "    llm_params = {\n",
    "        \"temperature\": 0,\n",
    "        \"model\": \"gpt-4o\",  # or \"gpt-3.5-turbo\" for cheaper option\n",
    "        # \"api_key\": \"your-api-key-here\"  # if not using environment variable\n",
    "    }\n",
    "    \n",
    "    # Create the tool instance\n",
    "    pdf_tool = PDFRetrievalTool(\n",
    "        documents_json_path=documents_json_path,\n",
    "        llm_params=llm_params\n",
    "    )\n",
    "    \n",
    "    print(\"✅ PDF Retrieval Tool initialized\")\n",
    "    return pdf_tool\n",
    "\n",
    "pdf_tool = setup_pdf_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248fd34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PDF RETRIEVAL TOOL - USAGE EXAMPLES\n",
      "============================================================\n",
      "\n",
      "1️⃣ LISTING AVAILABLE DOCUMENTS:\n",
      "----------------------------------------\n",
      "**Available Documents (16):**\n",
      "\n",
      "1. ❌ **Біліктілік санатын беруге (растауға) арналған Комиссия отырысының хаттамасы**\n",
      "   🏷️ Keywords: комиссия, растауға, арналған\n",
      "\n",
      "2. ❌ **комиссияның толық атауын көрсету аттестаттау комиссиясы отырысының хаттама**\n",
      "   🏷️ Keywords: хаттама, толық, көрсету\n",
      "\n",
      "3. ❌ **Педагогтің аттестаттау рәсіміне қатысуға өтініші**\n",
      "   🏷️ Keywords: өтініші, қатысуға, педагогтің\n",
      "\n",
      "4. ❌ **Аттестаттаудан өтуге өтінішті қабылдаудан бас тарту туралы хабарлама**\n",
      "   🏷️ Keywords: өтінішті, тарту, хабарлама\n",
      "\n",
      "5. ❌ **Біліктілік санатын беру (растау) туралы КУӘЛІК**\n",
      "   🏷️ Keywords: санатын, біліктілік, растау\n",
      "\n",
      "6. ❌ **Аттестаттаудан өтуге өтінішті қабылдау туралы хабарлама**\n",
      "   🏷️ Keywords: өтінішті, хабарлама, туралы\n",
      "\n",
      "7. ❌ **Педагогтердің білімін бағалаудан өткені туралы сертификат**\n",
      "   🏷️ Keywords: бағалаудан, өткені, сертификат\n",
      "\n",
      "8. ❌ **Педагогтердің білімін бағалауды өткізу ережелері мен шарттарын бұзу актісі**\n",
      "   🏷️ Keywords: бұзу, актісі, өткізу\n",
      "\n",
      "9. ❌ **Педагогтің білімін бағалаудан өтуге өтініш**\n",
      "   🏷️ Keywords: бағалаудан, өтініш, педагогтің\n",
      "\n",
      "10. ❌ **Уведомление о приеме заявления на прохождение аттестации**\n",
      "   🏷️ Keywords: приеме, уведомление, на\n",
      "\n",
      "11. ❌ **Заявление на прохождение аттестации педагога**\n",
      "   🏷️ Keywords: прохождение, на, заявление\n",
      "\n",
      "12. ❌ **Выписка из Протокола заседание аттестационной комиссии**\n",
      "   🏷️ Keywords: комиссии, заседание, протокола\n",
      "\n",
      "13. ❌ **Уведомление об отказе в дальнейшем рассмотрении заявления на прохождение аттестации**\n",
      "   🏷️ Keywords: рассмотрении, прохождение, уведомление\n",
      "\n",
      "14. ❌ **Протокол заседания Комиссии на присвоение (подтверждение) квалификационной категории**\n",
      "   🏷️ Keywords: подтверждение, комиссии, присвоение\n",
      "\n",
      "15. ❌ **Сертификат о прохождении оценки знаний педагога**\n",
      "   🏷️ Keywords: сертификат, о, знаний\n",
      "\n",
      "16. ❌ **Акт нарушения правил и условий проведения оценки знаний педагога**\n",
      "   🏷️ Keywords: проведения, и, правил\n",
      "\n",
      "2️⃣ DOCUMENT REQUEST EXAMPLES:\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PDF RETRIEVAL TOOL - USAGE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: List available documents\n",
    "print(\"\\n1️⃣ LISTING AVAILABLE DOCUMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "available_docs = pdf_tool.list_available_documents()\n",
    "print(available_docs)\n",
    "\n",
    "# Example 2: Direct document requests\n",
    "print(\"\\n2️⃣ DOCUMENT REQUEST EXAMPLES:\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72850a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\"Отправь мне акт о нарушений правил аттестации\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f906e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 16 document mappings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided to retrieve document: {'document_name': 'Акт нарушения правил и условий проведения оценки знаний педагога', 'search_keywords': ['нарушения', 'правил', 'условий', 'проведения', 'оценки', 'знаний', 'педагога'], 'document_type': 'акт'}\n"
     ]
    }
   ],
   "source": [
    "# For basic usage\n",
    "pdf_tool = PDFRetrievalTool(\"/workspaces/chatbot-rag-83/old_ones/documents.json\")\n",
    "response, metadata = pdf_tool.process_user_query(\"Отправь пожалуйста Акт нарушения правил и условий проведения оценки знаний педагога\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ad082e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Document 'Акт нарушения правил и условий проведения оценки знаний педагога' not found.\n",
      "\n",
      "💡 **Did you mean: Акт нарушения правил и условий проведения оценки знаний педагога, Сертификат о прохождении оценки знаний педагога, Заявление на прохождение аттестации педагога?**\n",
      "\n",
      "📋 **Available documents:** Біліктілік санатын беруге (растауға) арналған Комиссия отырысының хаттамасы, комиссияның толық атауын көрсету аттестаттау комиссиясы отырысының хаттама, Педагогтің аттестаттау рәсіміне қатысуға өтініші...\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a475170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_query': 'Отправь пожалуйста Акт нарушения правил и условий проведения оценки знаний педагога', 'document_requested': True, 'document_retrieved': False, 'total_cost': 0.0021000000000000003, 'available_documents_count': 16, 'decision_cost': {'total_tokens': 1148, 'prompt_tokens': 1080, 'completion_tokens': 68, 'total_cost': 0.0021000000000000003}, 'function_args': {'document_name': 'Акт нарушения правил и условий проведения оценки знаний педагога', 'search_keywords': ['нарушения', 'правил', 'условий', 'проведения', 'оценки', 'знаний', 'педагога'], 'document_type': 'акт'}, 'success': False, 'message': \"Document 'Акт нарушения правил и условий проведения оценки знаний педагога' not found.\", 'available_documents': ['Біліктілік санатын беруге (растауға) арналған Комиссия отырысының хаттамасы', 'комиссияның толық атауын көрсету аттестаттау комиссиясы отырысының хаттама', 'Педагогтің аттестаттау рәсіміне қатысуға өтініші', 'Аттестаттаудан өтуге өтінішті қабылдаудан бас тарту туралы хабарлама', 'Біліктілік санатын беру (растау) туралы КУӘЛІК', 'Аттестаттаудан өтуге өтінішті қабылдау туралы хабарлама', 'Педагогтердің білімін бағалаудан өткені туралы сертификат', 'Педагогтердің білімін бағалауды өткізу ережелері мен шарттарын бұзу актісі', 'Педагогтің білімін бағалаудан өтуге өтініш', 'Уведомление о приеме заявления на прохождение аттестации', 'Заявление на прохождение аттестации педагога', 'Выписка из Протокола заседание аттестационной комиссии', 'Уведомление об отказе в дальнейшем рассмотрении заявления на прохождение аттестации', 'Протокол заседания Комиссии на присвоение (подтверждение) квалификационной категории', 'Сертификат о прохождении оценки знаний педагога', 'Акт нарушения правил и условий проведения оценки знаний педагога'], 'suggestions': ['Акт нарушения правил и условий проведения оценки знаний педагога', 'Сертификат о прохождении оценки знаний педагога', 'Заявление на прохождение аттестации педагога', 'Протокол заседания Комиссии на присвоение (подтверждение) квалификационной категории', 'Біліктілік санатын беруге (растауға) арналған Комиссия отырысының хаттамасы'], 'suggestion_text': 'Did you mean: Акт нарушения правил и условий проведения оценки знаний педагога, Сертификат о прохождении оценки знаний педагога, Заявление на прохождение аттестации педагога?'}\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896a86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_queries = ['Какое решение принимает аттестационная комиссия, если отсутствует один из критериев на заявляемую категорию?',\n",
    " 'Можно ли сохранить действующую квалификационную категорию руководителю и заместителю руководителя за 4 года до выхода на пенсию?',\n",
    " 'Обязан ли педагог проходить ОЗП, если у него стаж более 30 лет?',\n",
    " 'Если в дипломе педагога два предмета, по какому из них он должен проходить аттестацию?',\n",
    " 'Сохраняется ли квалификационная категория педагога, если он переходит из дошкольной организации в организацию  среднего образования?',\n",
    " 'Какой пороговый уровень ОЗП?',\n",
    " 'Сколько баллов нужно педагогу-модератору для прохождения квалификационного теста?']\n",
    "\n",
    "example_queries_kk = ['Аттестациялық комиссия талап етілетін санат бойынша бір критерий болмаған жағдайда қандай шешім қабылдайды?',\n",
    "'Зейнеткерлікке шығуына 4 жыл қалғанда басшы мен басшының орынбасарына қолданыстағы біліктілік санатын сақтауға бола ма?',\n",
    "'Егер педагогтың еңбек өтілі 30 жылдан асса, ол міндетті ПББ-ден өтуі тиіс пе?',\n",
    "'Егер педагогтың дипломында екі пән көрсетілсе, ол қай пән бойынша аттестациядан өтуі керек?',\n",
    "'Педагог мектепке дейінгі ұйымнан орта білім беру ұйымына ауысқан жағдайда оның біліктілік санаты сақтала ма?',\n",
    "'ПББ үшін өту шегі қандай?',\n",
    "'Біліктілік тестінен өту үшін педагог-модератор қанша балл жинауы керек?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04488cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Аттестациялық комиссия талап етілетін санат бойынша бір критерий болмаған жағдайда қандай шешім қабылдайды?\n",
      "- Зейнеткерлікке шығуына 4 жыл қалғанда басшы мен басшының орынбасарына қолданыстағы біліктілік санатын сақтауға бола ма?\n",
      "- Егер педагогтың еңбек өтілі 30 жылдан асса, ол міндетті ПББ-ден өтуі тиіс пе?\n",
      "- Егер педагогтың дипломында екі пән көрсетілсе, ол қай пән бойынша аттестациядан өтуі керек?\n",
      "- Педагог мектепке дейінгі ұйымнан орта білім беру ұйымына ауысқан жағдайда оның біліктілік санаты сақтала ма?\n",
      "- ПББ үшін өту шегі қандай?\n",
      "- Біліктілік тестінен өту үшін педагог-модератор қанша балл жинауы керек?\n"
     ]
    }
   ],
   "source": [
    "for query in example_queries_kk:\n",
    "    print(f\"- {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fab07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f1f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspaces/chatbot-rag-83/documents/documents.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b42f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39dcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data.items():\n",
    "    new_key = key + '№ 83 Об утверждении Правил и условий проведения аттестации педагогических работников и приравненных к ним лиц, занимающих должности в организациях образования, реализующих общеобразовательные учебные программы дошкольного воспитания и обучения, начального, основного среднего и общего среднего образования, образовательные программы технического и профессионального, послесреднего, дополнительного образования и специальные учебные программы, иных гражданских служащих в области образования и науки '\n",
    "    new_data[new_key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6f043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspaces/chatbot-rag-83/documents/documents.json', 'w') as f:\n",
    "    json.dump(new_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c89fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Приложение 14: Лист наблюдения урока (занятия, организованной деятельности, мероприятия) педагога организации среднего (специального), дополнительного, технического и профессионального, послесреднего образования, организаций образования для детей-сирот и детей, оставшихся без попечения родителей'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
