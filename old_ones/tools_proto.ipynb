{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ade1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import your existing RAG fusion function\n",
    "from rag_pipeline.rag_fusion_pipeline import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SmartRAGTool:\n",
    "    def __init__(self, \n",
    "                 local_index_path: str,\n",
    "                 embedding_model,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Smart RAG Tool\n",
    "        \n",
    "        Args:\n",
    "            local_index_path: Path to the FAISS vector store\n",
    "            embedding_model: Embedding model for vector store\n",
    "            llm_params: Parameters for the LLM\n",
    "        \"\"\"\n",
    "        self.local_index_path = local_index_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "        self.llm = ChatOpenAI(**self.llm_params)\n",
    "        \n",
    "        # Function definition for OpenAI function calling\n",
    "        self.function_definition = {\n",
    "            \"name\": \"search_knowledge_base\",\n",
    "            \"description\": \"Search the knowledge base for specific information when the question requires domain-specific or detailed factual information that may not be in general knowledge\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to find relevant information\"\n",
    "                    },\n",
    "                    \"mode\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"original\", \"generated\"],\n",
    "                        \"description\": \"Search mode: 'original' uses only the user query, 'generated' creates multiple related queries for better coverage\"\n",
    "                    },\n",
    "                    \"num_queries\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"minimum\": 1,\n",
    "                        \"maximum\": 10,\n",
    "                        \"description\": \"Number of queries to generate if using 'generated' mode (default: 3)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Prompt to determine if RAG is needed\n",
    "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an AI assistant that decides whether a user question requires searching a knowledge base or can be answered with general knowledge.\n",
    "\n",
    "Use the search_knowledge_base function ONLY when:\n",
    "1. The questions is specific and realted to a policy, fact, legal specification.\n",
    "2. The question requires current or specific information that might not be in general knowledge\n",
    "\n",
    "DO NOT use the search function for:\n",
    "1. General knowledge questions (e.g., \"What is machine learning?\", \"How does photosynthesis work?\")\n",
    "2. Questions not related to the knowledge questions should be politely redirected to ask user for questions that are related to the knowledge base.\n",
    "\n",
    "If you decide to search, choose the appropriate mode:\n",
    "- Use \"original\" mode for simple, direct queries\n",
    "- Use \"generated\" mode for complex questions that might benefit from multiple search perspectives\n",
    "\n",
    "If you don't need to search, answer the question directly using your general knowledge.\"\"\"),\n",
    "            (\"user\", \"{user_query}\")\n",
    "        ])\n",
    "\n",
    "    def search_knowledge_base(self, query: str, mode: str = \"generated\", num_queries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Search the knowledge base using RAG fusion\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            mode: Search mode ('original' or 'generated')\n",
    "            num_queries: Number of queries to generate if using 'generated' mode\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Searching knowledge base with query: '{query}' in {mode} mode\")\n",
    "            \n",
    "            answer, metadata = rag_fusion_answer(\n",
    "                user_query=query,\n",
    "                local_index_path=self.local_index_path,\n",
    "                embedding_model=self.embedding_model,\n",
    "                mode=mode,\n",
    "                num_generated_queries=num_queries,\n",
    "                top_k=5,  # Retrieve more documents for better context\n",
    "                params=self.llm_params\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"metadata\": metadata,\n",
    "                \"search_performed\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in knowledge base search: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": f\"I encountered an error while searching the knowledge base: {str(e)}\",\n",
    "                \"metadata\": {},\n",
    "                \"search_performed\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def process_user_query(self, user_query: str, chat_context: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a user query and decide whether to use RAG or answer directly\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            chat_context: Optional conversation context\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (answer, metadata)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create the chain with function calling\n",
    "            chain = self.decision_prompt | self.llm.bind(\n",
    "                functions=[self.function_definition],\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "            \n",
    "            metadata = {\n",
    "                \"user_query\": user_query,\n",
    "                \"decision_made\": None,\n",
    "                \"search_performed\": False,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"token_usage\": {}\n",
    "            }\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain.invoke({\"user_query\": user_query})\n",
    "            \n",
    "            # Track decision-making cost\n",
    "            decision_cost = {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "            metadata[\"decision_cost\"] = decision_cost\n",
    "            metadata[\"total_cost\"] += cb.total_cost\n",
    "            \n",
    "            # Check if the model decided to use function calling\n",
    "            if hasattr(response, 'additional_kwargs') and 'function_call' in response.additional_kwargs:\n",
    "                function_call = response.additional_kwargs['function_call']\n",
    "                function_name = function_call['name']\n",
    "                function_args = json.loads(function_call['arguments'])\n",
    "                \n",
    "                logger.info(f\"LLM decided to use function: {function_name} with args: {function_args}\")\n",
    "                metadata[\"decision_made\"] = \"search_needed\"\n",
    "                \n",
    "                if function_name == \"search_knowledge_base\":\n",
    "                    # Execute the RAG search\n",
    "                    search_result = self.search_knowledge_base(\n",
    "                        query=function_args.get('query', user_query),\n",
    "                        mode=function_args.get('mode', 'generated'),\n",
    "                        num_queries=function_args.get('num_queries', 3)\n",
    "                    )\n",
    "                    \n",
    "                    if search_result.get(\"search_performed\"):\n",
    "                        metadata.update(search_result[\"metadata\"])\n",
    "                        metadata[\"search_performed\"] = True\n",
    "                        metadata[\"total_cost\"] += search_result[\"metadata\"].get(\"total_price\", 0)\n",
    "                        \n",
    "                        return search_result[\"answer\"], metadata\n",
    "                    else:\n",
    "                        # Fallback if search failed\n",
    "                        return f\"I tried to search for information but encountered an issue. Based on general knowledge: I'd be happy to help, but I may need more specific information to give you the most accurate answer.\", metadata\n",
    "                        \n",
    "            else:\n",
    "                # LLM decided not to search - use the direct response\n",
    "                logger.info(\"LLM decided no search needed, providing direct answer\")\n",
    "                metadata[\"decision_made\"] = \"direct_answer\"\n",
    "                return response.content, metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_user_query: {str(e)}\")\n",
    "            metadata[\"error\"] = str(e)\n",
    "            return f\"I encountered an error while processing your question: {str(e)}\", metadata\n",
    "\n",
    "    def chat(self, user_query: str, chat_context: Optional[str] = None, verbose: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Simple chat interface that handles the query and returns just the answer\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            chat_context: Optional conversation context\n",
    "            verbose: Whether to print detailed metadata\n",
    "            \n",
    "        Returns:\n",
    "            The answer string\n",
    "        \"\"\"\n",
    "        answer, metadata = self.process_user_query(user_query, chat_context)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Smart RAG Tool Execution Report ---\")\n",
    "            print(f\"User Query: {user_query}\")\n",
    "            print(f\"Decision Made: {metadata.get('decision_made', 'unknown')}\")\n",
    "            print(f\"Search Performed: {metadata.get('search_performed', False)}\")\n",
    "            print(f\"Total Cost: ${metadata.get('total_cost', 0):.4f}\")\n",
    "            \n",
    "            if metadata.get('search_performed'):\n",
    "                token_usage = metadata.get('token_usage', {})\n",
    "                print(f\"Total Tokens Used: {token_usage.get('total_tokens', 0)}\")\n",
    "                print(f\"Queries Used: {metadata.get('queries_used', [])}\")\n",
    "                print(f\"Documents Retrieved: {metadata.get('num_documents_retrieved', 0)}\")\n",
    "            \n",
    "            print(f\"--- End Report ---\\n\")\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "def create_smart_rag_tool(local_index_path: str, embedding_model) -> SmartRAGTool:\n",
    "    \"\"\"Factory function to create a Smart RAG Tool instance\"\"\"\n",
    "    return SmartRAGTool(\n",
    "        local_index_path=local_index_path,\n",
    "        embedding_model=embedding_model,\n",
    "        llm_params={\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "    # Initialize the tool\n",
    "    # smart_rag = create_smart_rag_tool(\"./faiss_index\", your_embedding_model)\n",
    "    \n",
    "    # Example questions that would trigger different behaviors:\n",
    "    \n",
    "    # This would likely NOT trigger RAG (general knowledge)\n",
    "    # answer = smart_rag.chat(\"What is machine learning?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # This would likely trigger RAG (specific/domain knowledge)\n",
    "    # answer = smart_rag.chat(\"What are the specific implementation details of our authentication system?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # This would likely trigger RAG with generated mode (complex query)\n",
    "    # answer = smart_rag.chat(\"How does our system handle user permissions and what are the security implications?\", verbose=True)\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7274cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "smart_rag = create_smart_rag_tool(\"./data/faiss_index\", OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided to use function: search_knowledge_base with args: {'query': '–ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –û–ó–ü', 'mode': 'original'}\n",
      "INFO:__main__:Searching knowledge base with query: '–ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –û–ó–ü' in original mode\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "INFO:rag_pipeline.rag_fusion_pipeline:Running in original query mode.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart RAG Tool Execution Report ---\n",
      "User Query: –ö–∞–∫–æ–π –ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –û–ó–ü\n",
      "Decision Made: search_needed\n",
      "Search Performed: True\n",
      "Total Cost: $0.0065\n",
      "Total Tokens Used: 1720\n",
      "Queries Used: ['–ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –û–ó–ü']\n",
      "Documents Retrieved: 4\n",
      "--- End Report ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'–ü–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–π –ø–µ–¥–∞–≥–æ–≥–æ–≤ (–û–ó–ü) –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∞. –°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, –ø–æ—Ä–æ–≥–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ:\\n\\n- –î–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´–ø–µ–¥–∞–≥–æ–≥-—Å—Ç–∞–∂–µ—Ä/–ø–µ–¥–∞–≥–æ–≥¬ª - 50%;\\n- –î–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´–ø–µ–¥–∞–≥–æ–≥-–º–æ–¥–µ—Ä–∞—Ç–æ—Ä¬ª - 60%;\\n- –î–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´–ø–µ–¥–∞–≥–æ–≥-—ç–∫—Å–ø–µ—Ä—Ç¬ª - 70%;\\n- –î–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´–ø–µ–¥–∞–≥–æ–≥-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å¬ª - 80%;\\n- –î–ª—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´–ø–µ–¥–∞–≥–æ–≥-–º–∞—Å—Ç–µ—Ä¬ª - 90%.\\n\\n–î–ª—è –ø–µ—Ä–≤—ã—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π, –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª–µ–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö –∫–∞–±–∏–Ω–µ—Ç–æ–≤ (—Ü–µ–Ω—Ç—Ä–æ–≤) –ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 70%.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = smart_rag.chat(\"–ö–∞–∫–æ–π –ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –û–ó–ü\", verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe0a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided no search needed, providing direct answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart RAG Tool Execution Report ---\n",
      "User Query: –ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏?\n",
      "Decision Made: direct_answer\n",
      "Search Performed: False\n",
      "Total Cost: $0.0009\n",
      "--- End Report ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'–°—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏ ‚Äî –ü–∞—Ä–∏–∂.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_rag.chat(\"–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9e8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.manager import get_openai_callback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFRetrievalTool:\n",
    "    def __init__(self, \n",
    "                 documents_json_path: str,\n",
    "                 llm_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize the PDF Retrieval Tool\n",
    "        \n",
    "        Args:\n",
    "            documents_json_path: Path to JSON file containing document mappings\n",
    "            llm_params: Parameters for the LLM\n",
    "        \"\"\"\n",
    "        self.documents_json_path = documents_json_path\n",
    "        self.llm_params = llm_params or {\"temperature\": 0, \"model\": \"gpt-4o\"}\n",
    "        self.llm = ChatOpenAI(**self.llm_params)\n",
    "        \n",
    "        # Load document mappings\n",
    "        self.document_mappings = self._load_document_mappings()\n",
    "        \n",
    "        # Create enhanced search indices\n",
    "        self.search_index = self._create_search_index()\n",
    "        \n",
    "        # Enhanced function definition for OpenAI function calling\n",
    "        self.function_definition = {\n",
    "            \"name\": \"retrieve_document\",\n",
    "            \"description\": \"Retrieve a specific document when user asks for any document, file, manual, guide, report, or wants to download/access/get any document\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"document_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name or identifier of the document to retrieve\"\n",
    "                    },\n",
    "                    \"search_keywords\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Keywords extracted from user query to help find the document\"\n",
    "                    },\n",
    "                    \"document_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Type of document (manual, guide, report, documentation, etc.)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"document_name\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # More aggressive prompt to catch document requests\n",
    "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are a document retrieval assistant. Your job is to identify when users want to access, download, or retrieve ANY document.\n",
    "\n",
    "AVAILABLE DOCUMENTS:\n",
    "{self._format_available_documents_detailed()}\n",
    "\n",
    "ALWAYS use the retrieve_document function when users:\n",
    "- Ask for ANY document, file, manual, guide, report, or PDF\n",
    "- Want to \"download\", \"get\", \"retrieve\", \"access\", \"show\", \"open\", or \"see\" a document\n",
    "- Ask questions like \"do you have...\", \"can I get...\", \"where is...\", \"find...\" followed by document-related terms\n",
    "- Mention specific document types (manual, guide, documentation, report, etc.)\n",
    "- Use phrases like \"I need the...\", \"show me the...\", \"give me the...\"\n",
    "\n",
    "EXAMPLES that should trigger retrieve_document:\n",
    "‚úÖ \"Can I get the user manual?\"\n",
    "‚úÖ \"I need the API documentation\"  \n",
    "‚úÖ \"Do you have the installation guide?\"\n",
    "‚úÖ \"Show me the quarterly report\"\n",
    "‚úÖ \"Where is the policy document?\"\n",
    "‚úÖ \"Find the technical specifications\"\n",
    "‚úÖ \"I want to see the handbook\"\n",
    "‚úÖ \"Download the reference guide\"\n",
    "‚úÖ \"Give me the troubleshooting manual\"\n",
    "‚úÖ \"Can you provide the system documentation?\"\n",
    "\n",
    "When calling retrieve_document:\n",
    "1. Extract the main document name/type from the user's request\n",
    "2. Include relevant keywords from their query in search_keywords\n",
    "3. Specify the document_type if identifiable\n",
    "\n",
    "Be very liberal in detecting document requests - when in doubt, assume they want a document.\"\"\"),\n",
    "            (\"user\", \"{user_query}\")\n",
    "        ])\n",
    "\n",
    "    def _load_document_mappings(self) -> Dict[str, str]:\n",
    "        \"\"\"Load document name to path mappings from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.documents_json_path, 'r', encoding='utf-8') as f:\n",
    "                mappings = json.load(f)\n",
    "            logger.info(f\"Loaded {len(mappings)} document mappings\")\n",
    "            return mappings\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Document mappings file not found: {self.documents_json_path}\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error parsing document mappings JSON: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _create_search_index(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Create enhanced search index with keywords for each document\"\"\"\n",
    "        search_index = {}\n",
    "        \n",
    "        for doc_name in self.document_mappings.keys():\n",
    "            # Extract keywords from document name\n",
    "            keywords = self._extract_keywords(doc_name)\n",
    "            search_index[doc_name] = keywords\n",
    "            \n",
    "        return search_index\n",
    "\n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract searchable keywords from text\"\"\"\n",
    "        # Convert to lowercase and split on common separators\n",
    "        text = text.lower()\n",
    "        # Split on various separators and remove empty strings\n",
    "        words = re.split(r'[_\\-\\s\\.\\(\\)\\[\\]]+', text)\n",
    "        words = [w.strip() for w in words if w.strip()]\n",
    "        \n",
    "        # Add common synonyms and variations\n",
    "        expanded_words = set(words)\n",
    "        \n",
    "        # Add synonyms for common terms\n",
    "        synonyms = {\n",
    "            'manual': ['guide', 'handbook', 'documentation', 'doc', 'instructions'],\n",
    "            'guide': ['manual', 'handbook', 'documentation', 'tutorial', 'howto'],\n",
    "            'api': ['interface', 'endpoint', 'service', 'programming'],\n",
    "            'install': ['installation', 'setup', 'deployment', 'configure'],\n",
    "            'user': ['users', 'customer', 'client'],\n",
    "            'admin': ['administrator', 'administration', 'management'],\n",
    "            'tech': ['technical', 'technology'],\n",
    "            'spec': ['specification', 'specifications', 'specs'],\n",
    "            'ref': ['reference', 'references'],\n",
    "            'trouble': ['troubleshooting', 'troubleshoot', 'debug', 'problem'],\n",
    "        }\n",
    "        \n",
    "        for word in words:\n",
    "            if word in synonyms:\n",
    "                expanded_words.update(synonyms[word])\n",
    "        \n",
    "        return list(expanded_words)\n",
    "\n",
    "    def _format_available_documents_detailed(self) -> str:\n",
    "        \"\"\"Format available documents with more detail for better matching\"\"\"\n",
    "        if not self.document_mappings:\n",
    "            return \"No documents currently available.\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            keywords = ', '.join(self.search_index.get(doc_name, [])[:5])  # Show first 5 keywords\n",
    "            status = \"‚úÖ\" if os.path.exists(doc_path) else \"‚ùå\"\n",
    "            doc_list.append(f\"{status} **{doc_name}**\\n   Keywords: {keywords}\")\n",
    "        \n",
    "        return \"\\n\".join(doc_list)\n",
    "\n",
    "    def _calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate similarity between two strings\"\"\"\n",
    "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "    def _find_document_enhanced(self, document_name: str, search_keywords: Optional[List[str]] = None, document_type: Optional[str] = None) -> Optional[Tuple[str, str, float]]:\n",
    "        \"\"\"\n",
    "        Enhanced document finding with multiple strategies\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (matched_name, file_path, confidence_score) or None\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        # Strategy 1: Exact name match\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            if doc_name.lower() == document_name.lower():\n",
    "                if os.path.exists(doc_path):\n",
    "                    return doc_name, doc_path, 1.0\n",
    "        \n",
    "        # Strategy 2: High similarity match\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            similarity = self._calculate_similarity(document_name, doc_name)\n",
    "            if similarity > 0.8 and os.path.exists(doc_path):\n",
    "                candidates.append((doc_name, doc_path, similarity))\n",
    "        \n",
    "        # Strategy 3: Partial name matching\n",
    "        document_name_lower = document_name.lower()\n",
    "        for doc_name, doc_path in self.document_mappings.items():\n",
    "            doc_name_lower = doc_name.lower()\n",
    "            if (document_name_lower in doc_name_lower or \n",
    "                doc_name_lower in document_name_lower or\n",
    "                any(word in doc_name_lower for word in document_name_lower.split() if len(word) > 2)):\n",
    "                if os.path.exists(doc_path):\n",
    "                    similarity = self._calculate_similarity(document_name, doc_name)\n",
    "                    candidates.append((doc_name, doc_path, similarity))\n",
    "        \n",
    "        # Strategy 4: Keyword matching\n",
    "        if search_keywords:\n",
    "            for doc_name, doc_path in self.document_mappings.items():\n",
    "                doc_keywords = self.search_index.get(doc_name, [])\n",
    "                keyword_matches = 0\n",
    "                for keyword in search_keywords:\n",
    "                    keyword_lower = keyword.lower()\n",
    "                    for doc_keyword in doc_keywords:\n",
    "                        if (keyword_lower == doc_keyword or \n",
    "                            keyword_lower in doc_keyword or \n",
    "                            doc_keyword in keyword_lower):\n",
    "                            keyword_matches += 1\n",
    "                            break\n",
    "                \n",
    "                if keyword_matches > 0 and os.path.exists(doc_path):\n",
    "                    # Calculate confidence based on keyword matches\n",
    "                    confidence = min(0.9, keyword_matches / len(search_keywords) * 0.8)\n",
    "                    candidates.append((doc_name, doc_path, confidence))\n",
    "        \n",
    "        # Strategy 5: Document type matching\n",
    "        if document_type:\n",
    "            document_type_lower = document_type.lower()\n",
    "            for doc_name, doc_path in self.document_mappings.items():\n",
    "                if document_type_lower in doc_name.lower() and os.path.exists(doc_path):\n",
    "                    similarity = self._calculate_similarity(document_type, doc_name)\n",
    "                    candidates.append((doc_name, doc_path, similarity * 0.7))\n",
    "        \n",
    "        # Remove duplicates and sort by confidence\n",
    "        unique_candidates = {}\n",
    "        for name, path, confidence in candidates:\n",
    "            if name not in unique_candidates or confidence > unique_candidates[name][2]:\n",
    "                unique_candidates[name] = (name, path, confidence)\n",
    "        \n",
    "        if unique_candidates:\n",
    "            # Return the best match\n",
    "            best_match = max(unique_candidates.values(), key=lambda x: x[2])\n",
    "            return best_match\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def retrieve_document(self, document_name: str, search_keywords: Optional[List[str]] = None, document_type: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced document retrieval with better matching\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self._find_document_enhanced(document_name, search_keywords, document_type)\n",
    "            \n",
    "            if result is None:\n",
    "                # Provide better suggestions\n",
    "                suggestions = self._get_suggestions(document_name, search_keywords, document_type)\n",
    "                available_docs = list(self.document_mappings.keys())\n",
    "                \n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"message\": f\"Document '{document_name}' not found.\",\n",
    "                    \"available_documents\": available_docs,\n",
    "                    \"suggestions\": suggestions,\n",
    "                    \"suggestion_text\": f\"Did you mean: {', '.join(suggestions[:3])}?\" if suggestions else \"Please check available documents.\"\n",
    "                }\n",
    "            \n",
    "            matched_name, file_path, confidence = result\n",
    "            \n",
    "            # Get file info\n",
    "            file_stat = os.stat(file_path)\n",
    "            file_size = file_stat.st_size\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            # Get MIME type\n",
    "            mime_type, _ = mimetypes.guess_type(file_path)\n",
    "            \n",
    "            logger.info(f\"Successfully retrieved document: {matched_name} (confidence: {confidence:.2f}, size: {file_size_mb:.2f} MB)\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"document_name\": matched_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"file_size\": file_size,\n",
    "                \"file_size_mb\": round(file_size_mb, 2),\n",
    "                \"mime_type\": mime_type or \"application/pdf\",\n",
    "                \"confidence\": round(confidence, 2),\n",
    "                \"message\": f\"Document '{matched_name}' is ready for download (confidence: {confidence:.1%}).\",\n",
    "                \"download_info\": {\n",
    "                    \"filename\": os.path.basename(file_path),\n",
    "                    \"extension\": Path(file_path).suffix\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving document '{document_name}': {str(e)}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": f\"Error retrieving document: {str(e)}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def _get_suggestions(self, document_name: str, search_keywords: Optional[List[str]] = None, document_type: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Get document suggestions based on partial matches\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        # Find partial matches\n",
    "        for doc_name in self.document_mappings.keys():\n",
    "            similarity = self._calculate_similarity(document_name, doc_name)\n",
    "            if similarity > 0.3:  # Lower threshold for suggestions\n",
    "                suggestions.append((doc_name, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top suggestions\n",
    "        suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [name for name, _ in suggestions[:5]]\n",
    "\n",
    "    def process_user_query(self, user_query: str) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Enhanced query processing with better document detection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Force function calling to be more aggressive\n",
    "            chain = self.decision_prompt | self.llm.bind(\n",
    "                functions=[self.function_definition],\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "            \n",
    "            metadata = {\n",
    "                \"user_query\": user_query,\n",
    "                \"document_requested\": False,\n",
    "                \"document_retrieved\": False,\n",
    "                \"total_cost\": 0.0,\n",
    "                \"available_documents_count\": len(self.document_mappings)\n",
    "            }\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain.invoke({\"user_query\": user_query})\n",
    "            \n",
    "            # Track costs\n",
    "            metadata[\"decision_cost\"] = {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "            metadata[\"total_cost\"] += cb.total_cost\n",
    "            \n",
    "            # Check for function call in response\n",
    "            function_call = None\n",
    "            if hasattr(response, 'additional_kwargs') and 'function_call' in response.additional_kwargs:\n",
    "                function_call = response.additional_kwargs['function_call']\n",
    "            elif hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "                # Handle newer OpenAI API format\n",
    "                function_call = response.tool_calls[0].function if response.tool_calls else None\n",
    "            \n",
    "            if function_call:\n",
    "                try:\n",
    "                    function_name = function_call.name if hasattr(function_call, 'name') else function_call['name']\n",
    "                    function_args = json.loads(function_call.arguments if hasattr(function_call, 'arguments') else function_call['arguments'])\n",
    "                    \n",
    "                    logger.info(f\"LLM decided to retrieve document: {function_args}\")\n",
    "                    metadata[\"document_requested\"] = True\n",
    "                    metadata[\"function_args\"] = function_args\n",
    "                    \n",
    "                    if function_name == \"retrieve_document\":\n",
    "                        # Execute enhanced document retrieval\n",
    "                        retrieval_result = self.retrieve_document(\n",
    "                            document_name=function_args.get('document_name', ''),\n",
    "                            search_keywords=function_args.get('search_keywords', []),\n",
    "                            document_type=function_args.get('document_type', '')\n",
    "                        )\n",
    "                        \n",
    "                        metadata.update(retrieval_result)\n",
    "                        metadata[\"document_retrieved\"] = retrieval_result.get(\"success\", False)\n",
    "                        \n",
    "                        if retrieval_result.get(\"success\"):\n",
    "                            doc_info = retrieval_result\n",
    "                            response_message = (\n",
    "                                f\"‚úÖ **{doc_info['document_name']}** found and ready for download!\\n\\n\"\n",
    "                                f\"üìÑ **File:** {doc_info['download_info']['filename']}\\n\"\n",
    "                                f\"üìä **Size:** {doc_info['file_size_mb']} MB\\n\"\n",
    "                                f\"üéØ **Confidence:** {doc_info['confidence']:.1%}\\n\"\n",
    "                                f\"üìÅ **Location:** {doc_info['file_path']}\\n\\n\"\n",
    "                                f\"The document is ready for download from the specified location.\"\n",
    "                            )\n",
    "                        else:\n",
    "                            response_message = (\n",
    "                                f\"‚ùå {retrieval_result['message']}\\n\\n\"\n",
    "                                f\"üí° **{retrieval_result.get('suggestion_text', '')}**\\n\\n\"\n",
    "                                f\"üìã **Available documents:** {', '.join(list(self.document_mappings.keys())[:3])}...\"\n",
    "                            )\n",
    "                        \n",
    "                        return response_message, metadata\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing function call: {e}\")\n",
    "                    metadata[\"error\"] = str(e)\n",
    "            \n",
    "            # If no function call, provide helpful response\n",
    "            response_message = (\n",
    "                f\"I didn't detect a specific document request in your message. \"\n",
    "                f\"I have access to {len(self.document_mappings)} documents. \"\n",
    "                f\"Try asking something like:\\n\"\n",
    "                f\"‚Ä¢ 'Can I get the [document name]?'\\n\"\n",
    "                f\"‚Ä¢ 'I need the [type] documentation'\\n\"\n",
    "                f\"‚Ä¢ 'Show me the [document] guide'\\n\\n\"\n",
    "                f\"Available documents: {', '.join(list(self.document_mappings.keys())[:3])}...\"\n",
    "            )\n",
    "            return response_message, metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_user_query: {str(e)}\")\n",
    "            metadata[\"error\"] = str(e)\n",
    "            return f\"I encountered an error while processing your request: {str(e)}\", metadata\n",
    "\n",
    "    def list_available_documents(self) -> str:\n",
    "        \"\"\"Return a formatted list of available documents with enhanced info\"\"\"\n",
    "        if not self.document_mappings:\n",
    "            return \"No documents are currently available.\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for i, (doc_name, doc_path) in enumerate(self.document_mappings.items(), 1):\n",
    "            file_exists = \"‚úÖ\" if os.path.exists(doc_path) else \"‚ùå\"\n",
    "            \n",
    "            # Get file size if exists\n",
    "            size_info = \"\"\n",
    "            if os.path.exists(doc_path):\n",
    "                try:\n",
    "                    size_mb = os.path.getsize(doc_path) / (1024 * 1024)\n",
    "                    size_info = f\" ({size_mb:.1f} MB)\"\n",
    "                except:\n",
    "                    size_info = \"\"\n",
    "            \n",
    "            # Get keywords\n",
    "            keywords = ', '.join(self.search_index.get(doc_name, [])[:3])\n",
    "            \n",
    "            doc_list.append(\n",
    "                f\"{i}. {file_exists} **{doc_name}**{size_info}\\n\"\n",
    "                f\"   üè∑Ô∏è Keywords: {keywords}\"\n",
    "            )\n",
    "        \n",
    "        return f\"**Available Documents ({len(self.document_mappings)}):**\\n\\n\" + \"\\n\\n\".join(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811838af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 16 document mappings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF Retrieval Tool initialized\n"
     ]
    }
   ],
   "source": [
    "def setup_pdf_tool():\n",
    "    \"\"\"Set up the PDF retrieval tool\"\"\"\n",
    "    \n",
    "    # Create documents.json if it doesn't exist\n",
    "    documents_json_path = \"documents.json\"\n",
    "    if not os.path.exists(documents_json_path):\n",
    "        documents_json_path = create_documents_json()\n",
    "    \n",
    "    # Initialize the too\n",
    "    \n",
    "    # Configure LLM parameters (make sure you have OPENAI_API_KEY set)\n",
    "    llm_params = {\n",
    "        \"temperature\": 0,\n",
    "        \"model\": \"gpt-4o\",  # or \"gpt-3.5-turbo\" for cheaper option\n",
    "        # \"api_key\": \"your-api-key-here\"  # if not using environment variable\n",
    "    }\n",
    "    \n",
    "    # Create the tool instance\n",
    "    pdf_tool = PDFRetrievalTool(\n",
    "        documents_json_path=documents_json_path,\n",
    "        llm_params=llm_params\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ PDF Retrieval Tool initialized\")\n",
    "    return pdf_tool\n",
    "\n",
    "pdf_tool = setup_pdf_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248fd34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PDF RETRIEVAL TOOL - USAGE EXAMPLES\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ LISTING AVAILABLE DOCUMENTS:\n",
      "----------------------------------------\n",
      "**Available Documents (16):**\n",
      "\n",
      "1. ‚ùå **–ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω –±–µ—Ä—É–≥–µ (—Ä–∞—Å—Ç–∞—É“ì–∞) –∞—Ä–Ω–∞–ª“ì–∞–Ω –ö–æ–º–∏—Å—Å–∏—è –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞—Å—ã**\n",
      "   üè∑Ô∏è Keywords: –∫–æ–º–∏—Å—Å–∏—è, —Ä–∞—Å—Ç–∞—É“ì–∞, –∞—Ä–Ω–∞–ª“ì–∞–Ω\n",
      "\n",
      "2. ‚ùå **–∫–æ–º–∏—Å—Å–∏—è–Ω—ã“£ —Ç–æ–ª—ã“õ –∞—Ç–∞—É—ã–Ω –∫”©—Ä—Å–µ—Ç—É –∞—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É –∫–æ–º–∏—Å—Å–∏—è—Å—ã –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞**\n",
      "   üè∑Ô∏è Keywords: —Ö–∞—Ç—Ç–∞–º–∞, —Ç–æ–ª—ã“õ, –∫”©—Ä—Å–µ—Ç—É\n",
      "\n",
      "3. ‚ùå **–ü–µ–¥–∞–≥–æ–≥—Ç—ñ“£ –∞—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É —Ä”ô—Å—ñ–º—ñ–Ω–µ “õ–∞—Ç—ã—Å—É“ì–∞ ”©—Ç—ñ–Ω—ñ—à—ñ**\n",
      "   üè∑Ô∏è Keywords: ”©—Ç—ñ–Ω—ñ—à—ñ, “õ–∞—Ç—ã—Å—É“ì–∞, –ø–µ–¥–∞–≥–æ–≥—Ç—ñ“£\n",
      "\n",
      "4. ‚ùå **–ê—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É–¥–∞–Ω ”©—Ç—É–≥–µ ”©—Ç—ñ–Ω—ñ—à—Ç—ñ “õ–∞–±—ã–ª–¥–∞—É–¥–∞–Ω –±–∞—Å —Ç–∞—Ä—Ç—É —Ç—É—Ä–∞–ª—ã —Ö–∞–±–∞—Ä–ª–∞–º–∞**\n",
      "   üè∑Ô∏è Keywords: ”©—Ç—ñ–Ω—ñ—à—Ç—ñ, —Ç–∞—Ä—Ç—É, —Ö–∞–±–∞—Ä–ª–∞–º–∞\n",
      "\n",
      "5. ‚ùå **–ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω –±–µ—Ä—É (—Ä–∞—Å—Ç–∞—É) —Ç—É—Ä–∞–ª—ã –ö–£”ò–õ–Ü–ö**\n",
      "   üè∑Ô∏è Keywords: —Å–∞–Ω–∞—Ç—ã–Ω, –±—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫, —Ä–∞—Å—Ç–∞—É\n",
      "\n",
      "6. ‚ùå **–ê—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É–¥–∞–Ω ”©—Ç—É–≥–µ ”©—Ç—ñ–Ω—ñ—à—Ç—ñ “õ–∞–±—ã–ª–¥–∞—É —Ç—É—Ä–∞–ª—ã —Ö–∞–±–∞—Ä–ª–∞–º–∞**\n",
      "   üè∑Ô∏è Keywords: ”©—Ç—ñ–Ω—ñ—à—Ç—ñ, —Ö–∞–±–∞—Ä–ª–∞–º–∞, —Ç—É—Ä–∞–ª—ã\n",
      "\n",
      "7. ‚ùå **–ü–µ–¥–∞–≥–æ–≥—Ç–µ—Ä–¥—ñ“£ –±—ñ–ª—ñ–º—ñ–Ω –±–∞“ì–∞–ª–∞—É–¥–∞–Ω ”©—Ç–∫–µ–Ω—ñ —Ç—É—Ä–∞–ª—ã —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç**\n",
      "   üè∑Ô∏è Keywords: –±–∞“ì–∞–ª–∞—É–¥–∞–Ω, ”©—Ç–∫–µ–Ω—ñ, —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç\n",
      "\n",
      "8. ‚ùå **–ü–µ–¥–∞–≥–æ–≥—Ç–µ—Ä–¥—ñ“£ –±—ñ–ª—ñ–º—ñ–Ω –±–∞“ì–∞–ª–∞—É–¥—ã ”©—Ç–∫—ñ–∑—É –µ—Ä–µ–∂–µ–ª–µ—Ä—ñ –º–µ–Ω —à–∞—Ä—Ç—Ç–∞—Ä—ã–Ω –±“±–∑—É –∞–∫—Ç—ñ—Å—ñ**\n",
      "   üè∑Ô∏è Keywords: –±“±–∑—É, –∞–∫—Ç—ñ—Å—ñ, ”©—Ç–∫—ñ–∑—É\n",
      "\n",
      "9. ‚ùå **–ü–µ–¥–∞–≥–æ–≥—Ç—ñ“£ –±—ñ–ª—ñ–º—ñ–Ω –±–∞“ì–∞–ª–∞—É–¥–∞–Ω ”©—Ç—É–≥–µ ”©—Ç—ñ–Ω—ñ—à**\n",
      "   üè∑Ô∏è Keywords: –±–∞“ì–∞–ª–∞—É–¥–∞–Ω, ”©—Ç—ñ–Ω—ñ—à, –ø–µ–¥–∞–≥–æ–≥—Ç—ñ“£\n",
      "\n",
      "10. ‚ùå **–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –ø—Ä–∏–µ–º–µ –∑–∞—è–≤–ª–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏**\n",
      "   üè∑Ô∏è Keywords: –ø—Ä–∏–µ–º–µ, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–∞\n",
      "\n",
      "11. ‚ùå **–ó–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∞**\n",
      "   üè∑Ô∏è Keywords: –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ, –Ω–∞, –∑–∞—è–≤–ª–µ–Ω–∏–µ\n",
      "\n",
      "12. ‚ùå **–í—ã–ø–∏—Å–∫–∞ –∏–∑ –ü—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞—Å–µ–¥–∞–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–æ–Ω–Ω–æ–∏ÃÜ –∫–æ–º–∏—Å—Å–∏–∏**\n",
      "   üè∑Ô∏è Keywords: –∫–æ–º–∏—Å—Å–∏–∏, –∑–∞—Å–µ–¥–∞–Ω–∏–µ, –ø—Ä–æ—Ç–æ–∫–æ–ª–∞\n",
      "\n",
      "13. ‚ùå **–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—Ç–∫–∞–∑–µ –≤ –¥–∞–ª—å–Ω–µ–∏ÃÜ—à–µ–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–∏ –∑–∞—è–≤–ª–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏**\n",
      "   üè∑Ô∏è Keywords: —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–∏, –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ\n",
      "\n",
      "14. ‚ùå **–ü—Ä–æ—Ç–æ–∫–æ–ª –∑–∞—Å–µ–¥–∞–Ω–∏—è –ö–æ–º–∏—Å—Å–∏–∏ –Ω–∞ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ) –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–∏ÃÜ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏**\n",
      "   üè∑Ô∏è Keywords: –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ, –∫–æ–º–∏—Å—Å–∏–∏, –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ\n",
      "\n",
      "15. ‚ùå **–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞**\n",
      "   üè∑Ô∏è Keywords: —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç, –æ, –∑–Ω–∞–Ω–∏–∏ÃÜ\n",
      "\n",
      "16. ‚ùå **–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞**\n",
      "   üè∑Ô∏è Keywords: –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è, –∏, –ø—Ä–∞–≤–∏–ª\n",
      "\n",
      "2Ô∏è‚É£ DOCUMENT REQUEST EXAMPLES:\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PDF RETRIEVAL TOOL - USAGE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: List available documents\n",
    "print(\"\\n1Ô∏è‚É£ LISTING AVAILABLE DOCUMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "available_docs = pdf_tool.list_available_documents()\n",
    "print(available_docs)\n",
    "\n",
    "# Example 2: Direct document requests\n",
    "print(\"\\n2Ô∏è‚É£ DOCUMENT REQUEST EXAMPLES:\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72850a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\"–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –∞–∫—Ç –æ –Ω–∞—Ä—É—à–µ–Ω–∏–π –ø—Ä–∞–≤–∏–ª –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f906e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 16 document mappings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:LLM decided to retrieve document: {'document_name': '–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞', 'search_keywords': ['–Ω–∞—Ä—É—à–µ–Ω–∏—è', '–ø—Ä–∞–≤–∏–ª', '—É—Å–ª–æ–≤–∏–∏ÃÜ', '–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è', '–æ—Ü–µ–Ω–∫–∏', '–∑–Ω–∞–Ω–∏–∏ÃÜ', '–ø–µ–¥–∞–≥–æ–≥–∞'], 'document_type': '–∞–∫—Ç'}\n"
     ]
    }
   ],
   "source": [
    "# For basic usage\n",
    "pdf_tool = PDFRetrievalTool(\"/workspaces/chatbot-rag-83/old_ones/documents.json\")\n",
    "response, metadata = pdf_tool.process_user_query(\"–û—Ç–ø—Ä–∞–≤—å –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ad082e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Document '–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞' not found.\n",
      "\n",
      "üí° **Did you mean: –ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞, –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞, –ó–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∞?**\n",
      "\n",
      "üìã **Available documents:** –ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω –±–µ—Ä—É–≥–µ (—Ä–∞—Å—Ç–∞—É“ì–∞) –∞—Ä–Ω–∞–ª“ì–∞–Ω –ö–æ–º–∏—Å—Å–∏—è –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞—Å—ã, –∫–æ–º–∏—Å—Å–∏—è–Ω—ã“£ —Ç–æ–ª—ã“õ –∞—Ç–∞—É—ã–Ω –∫”©—Ä—Å–µ—Ç—É –∞—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É –∫–æ–º–∏—Å—Å–∏—è—Å—ã –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞, –ü–µ–¥–∞–≥–æ–≥—Ç—ñ“£ –∞—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É —Ä”ô—Å—ñ–º—ñ–Ω–µ “õ–∞—Ç—ã—Å—É“ì–∞ ”©—Ç—ñ–Ω—ñ—à—ñ...\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a475170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_query': '–û—Ç–ø—Ä–∞–≤—å –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞', 'document_requested': True, 'document_retrieved': False, 'total_cost': 0.0021000000000000003, 'available_documents_count': 16, 'decision_cost': {'total_tokens': 1148, 'prompt_tokens': 1080, 'completion_tokens': 68, 'total_cost': 0.0021000000000000003}, 'function_args': {'document_name': '–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞', 'search_keywords': ['–Ω–∞—Ä—É—à–µ–Ω–∏—è', '–ø—Ä–∞–≤–∏–ª', '—É—Å–ª–æ–≤–∏–∏ÃÜ', '–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è', '–æ—Ü–µ–Ω–∫–∏', '–∑–Ω–∞–Ω–∏–∏ÃÜ', '–ø–µ–¥–∞–≥–æ–≥–∞'], 'document_type': '–∞–∫—Ç'}, 'success': False, 'message': \"Document '–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞' not found.\", 'available_documents': ['–ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω –±–µ—Ä—É–≥–µ (—Ä–∞—Å—Ç–∞—É“ì–∞) –∞—Ä–Ω–∞–ª“ì–∞–Ω –ö–æ–º–∏—Å—Å–∏—è –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞—Å—ã', '–∫–æ–º–∏—Å—Å–∏—è–Ω—ã“£ —Ç–æ–ª—ã“õ –∞—Ç–∞—É—ã–Ω –∫”©—Ä—Å–µ—Ç—É –∞—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É –∫–æ–º–∏—Å—Å–∏—è—Å—ã –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞', '–ü–µ–¥–∞–≥–æ–≥—Ç—ñ“£ –∞—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É —Ä”ô—Å—ñ–º—ñ–Ω–µ “õ–∞—Ç—ã—Å—É“ì–∞ ”©—Ç—ñ–Ω—ñ—à—ñ', '–ê—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É–¥–∞–Ω ”©—Ç—É–≥–µ ”©—Ç—ñ–Ω—ñ—à—Ç—ñ “õ–∞–±—ã–ª–¥–∞—É–¥–∞–Ω –±–∞—Å —Ç–∞—Ä—Ç—É —Ç—É—Ä–∞–ª—ã —Ö–∞–±–∞—Ä–ª–∞–º–∞', '–ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω –±–µ—Ä—É (—Ä–∞—Å—Ç–∞—É) —Ç—É—Ä–∞–ª—ã –ö–£”ò–õ–Ü–ö', '–ê—Ç—Ç–µ—Å—Ç–∞—Ç—Ç–∞—É–¥–∞–Ω ”©—Ç—É–≥–µ ”©—Ç—ñ–Ω—ñ—à—Ç—ñ “õ–∞–±—ã–ª–¥–∞—É —Ç—É—Ä–∞–ª—ã —Ö–∞–±–∞—Ä–ª–∞–º–∞', '–ü–µ–¥–∞–≥–æ–≥—Ç–µ—Ä–¥—ñ“£ –±—ñ–ª—ñ–º—ñ–Ω –±–∞“ì–∞–ª–∞—É–¥–∞–Ω ”©—Ç–∫–µ–Ω—ñ —Ç—É—Ä–∞–ª—ã —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç', '–ü–µ–¥–∞–≥–æ–≥—Ç–µ—Ä–¥—ñ“£ –±—ñ–ª—ñ–º—ñ–Ω –±–∞“ì–∞–ª–∞—É–¥—ã ”©—Ç–∫—ñ–∑—É –µ—Ä–µ–∂–µ–ª–µ—Ä—ñ –º–µ–Ω —à–∞—Ä—Ç—Ç–∞—Ä—ã–Ω –±“±–∑—É –∞–∫—Ç—ñ—Å—ñ', '–ü–µ–¥–∞–≥–æ–≥—Ç—ñ“£ –±—ñ–ª—ñ–º—ñ–Ω –±–∞“ì–∞–ª–∞—É–¥–∞–Ω ”©—Ç—É–≥–µ ”©—Ç—ñ–Ω—ñ—à', '–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –ø—Ä–∏–µ–º–µ –∑–∞—è–≤–ª–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏', '–ó–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∞', '–í—ã–ø–∏—Å–∫–∞ –∏–∑ –ü—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞—Å–µ–¥–∞–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–æ–Ω–Ω–æ–∏ÃÜ –∫–æ–º–∏—Å—Å–∏–∏', '–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—Ç–∫–∞–∑–µ –≤ –¥–∞–ª—å–Ω–µ–∏ÃÜ—à–µ–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–∏ –∑–∞—è–≤–ª–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏', '–ü—Ä–æ—Ç–æ–∫–æ–ª –∑–∞—Å–µ–¥–∞–Ω–∏—è –ö–æ–º–∏—Å—Å–∏–∏ –Ω–∞ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ) –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–∏ÃÜ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏', '–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞', '–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞'], 'suggestions': ['–ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞', '–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞', '–ó–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∞', '–ü—Ä–æ—Ç–æ–∫–æ–ª –∑–∞—Å–µ–¥–∞–Ω–∏—è –ö–æ–º–∏—Å—Å–∏–∏ –Ω–∞ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ) –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–∏ÃÜ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏', '–ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω –±–µ—Ä—É–≥–µ (—Ä–∞—Å—Ç–∞—É“ì–∞) –∞—Ä–Ω–∞–ª“ì–∞–Ω –ö–æ–º–∏—Å—Å–∏—è –æ—Ç—ã—Ä—ã—Å—ã–Ω—ã“£ —Ö–∞—Ç—Ç–∞–º–∞—Å—ã'], 'suggestion_text': 'Did you mean: –ê–∫—Ç –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–∏ÃÜ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞, –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–∏ÃÜ –ø–µ–¥–∞–≥–æ–≥–∞, –ó–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∞?'}\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896a86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_queries = ['–ö–∞–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–æ–Ω–Ω–∞—è –∫–æ–º–∏—Å—Å–∏—è, –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–¥–∏–Ω –∏–∑ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –Ω–∞ –∑–∞—è–≤–ª—è–µ–º—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é?',\n",
    " '–ú–æ–∂–Ω–æ –ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤—É—é—â—É—é –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—é –∏ –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—é —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è –∑–∞ 4 –≥–æ–¥–∞ –¥–æ –≤—ã—Ö–æ–¥–∞ –Ω–∞ –ø–µ–Ω—Å–∏—é?',\n",
    " '–û–±—è–∑–∞–Ω –ª–∏ –ø–µ–¥–∞–≥–æ–≥ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –û–ó–ü, –µ—Å–ª–∏ —É –Ω–µ–≥–æ —Å—Ç–∞–∂ –±–æ–ª–µ–µ 30 –ª–µ—Ç?',\n",
    " '–ï—Å–ª–∏ –≤ –¥–∏–ø–ª–æ–º–µ –ø–µ–¥–∞–≥–æ–≥–∞ –¥–≤–∞ –ø—Ä–µ–¥–º–µ—Ç–∞, –ø–æ –∫–∞–∫–æ–º—É –∏–∑ –Ω–∏—Ö –æ–Ω –¥–æ–ª–∂–µ–Ω –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—é?',\n",
    " '–°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ª–∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –ø–µ–¥–∞–≥–æ–≥–∞, –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∏–∑ –¥–æ—à–∫–æ–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é  —Å—Ä–µ–¥–Ω–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è?',\n",
    " '–ö–∞–∫–æ–π –ø–æ—Ä–æ–≥–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –û–ó–ü?',\n",
    " '–°–∫–æ–ª—å–∫–æ –±–∞–ª–ª–æ–≤ –Ω—É–∂–Ω–æ –ø–µ–¥–∞–≥–æ–≥—É-–º–æ–¥–µ—Ä–∞—Ç–æ—Ä—É –¥–ª—è –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞?']\n",
    "\n",
    "example_queries_kk = ['–ê—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è–ª—ã“õ –∫–æ–º–∏—Å—Å–∏—è —Ç–∞–ª–∞–ø –µ—Ç—ñ–ª–µ—Ç—ñ–Ω —Å–∞–Ω–∞—Ç –±–æ–π—ã–Ω—à–∞ –±—ñ—Ä –∫—Ä–∏—Ç–µ—Ä–∏–π –±–æ–ª–º–∞“ì–∞–Ω –∂–∞“ì–¥–∞–π–¥–∞ “õ–∞–Ω–¥–∞–π —à–µ—à—ñ–º “õ–∞–±—ã–ª–¥–∞–π–¥—ã?',\n",
    "'–ó–µ–π–Ω–µ—Ç–∫–µ—Ä–ª—ñ–∫–∫–µ —à—ã“ì—É—ã–Ω–∞ 4 –∂—ã–ª “õ–∞–ª“ì–∞–Ω–¥–∞ –±–∞—Å—à—ã –º–µ–Ω –±–∞—Å—à—ã–Ω—ã“£ –æ—Ä—ã–Ω–±–∞—Å–∞—Ä—ã–Ω–∞ “õ–æ–ª–¥–∞–Ω—ã—Å—Ç–∞“ì—ã –±—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω —Å–∞“õ—Ç–∞—É“ì–∞ –±–æ–ª–∞ –º–∞?',\n",
    "'–ï–≥–µ—Ä –ø–µ–¥–∞–≥–æ–≥—Ç—ã“£ –µ“£–±–µ–∫ ”©—Ç—ñ–ª—ñ 30 –∂—ã–ª–¥–∞–Ω –∞—Å—Å–∞, –æ–ª –º—ñ–Ω–¥–µ—Ç—Ç—ñ –ü–ë–ë-–¥–µ–Ω ”©—Ç—É—ñ —Ç–∏—ñ—Å –ø–µ?',\n",
    "'–ï–≥–µ—Ä –ø–µ–¥–∞–≥–æ–≥—Ç—ã“£ –¥–∏–ø–ª–æ–º—ã–Ω–¥–∞ –µ–∫—ñ –ø”ô–Ω –∫”©—Ä—Å–µ—Ç—ñ–ª—Å–µ, –æ–ª “õ–∞–π –ø”ô–Ω –±–æ–π—ã–Ω—à–∞ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è–¥–∞–Ω ”©—Ç—É—ñ –∫–µ—Ä–µ–∫?',\n",
    "'–ü–µ–¥–∞–≥–æ–≥ –º–µ–∫—Ç–µ–ø–∫–µ –¥–µ–π—ñ–Ω–≥—ñ “±–π—ã–º–Ω–∞–Ω –æ—Ä—Ç–∞ –±—ñ–ª—ñ–º –±–µ—Ä—É “±–π—ã–º—ã–Ω–∞ –∞—É—ã—Å“õ–∞–Ω –∂–∞“ì–¥–∞–π–¥–∞ –æ–Ω—ã“£ –±—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã —Å–∞“õ—Ç–∞–ª–∞ –º–∞?',\n",
    "'–ü–ë–ë “Ø—à—ñ–Ω ”©—Ç—É —à–µ–≥—ñ “õ–∞–Ω–¥–∞–π?',\n",
    "'–ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Ç–µ—Å—Ç—ñ–Ω–µ–Ω ”©—Ç—É “Ø—à—ñ–Ω –ø–µ–¥–∞–≥–æ–≥-–º–æ–¥–µ—Ä–∞—Ç–æ—Ä “õ–∞–Ω—à–∞ –±–∞–ª–ª –∂–∏–Ω–∞—É—ã –∫–µ—Ä–µ–∫?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04488cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- –ê—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è–ª—ã“õ –∫–æ–º–∏—Å—Å–∏—è —Ç–∞–ª–∞–ø –µ—Ç—ñ–ª–µ—Ç—ñ–Ω —Å–∞–Ω–∞—Ç –±–æ–π—ã–Ω—à–∞ –±—ñ—Ä –∫—Ä–∏—Ç–µ—Ä–∏–π –±–æ–ª–º–∞“ì–∞–Ω –∂–∞“ì–¥–∞–π–¥–∞ “õ–∞–Ω–¥–∞–π —à–µ—à—ñ–º “õ–∞–±—ã–ª–¥–∞–π–¥—ã?\n",
      "- –ó–µ–π–Ω–µ—Ç–∫–µ—Ä–ª—ñ–∫–∫–µ —à—ã“ì—É—ã–Ω–∞ 4 –∂—ã–ª “õ–∞–ª“ì–∞–Ω–¥–∞ –±–∞—Å—à—ã –º–µ–Ω –±–∞—Å—à—ã–Ω—ã“£ –æ—Ä—ã–Ω–±–∞—Å–∞—Ä—ã–Ω–∞ “õ–æ–ª–¥–∞–Ω—ã—Å—Ç–∞“ì—ã –±—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã–Ω —Å–∞“õ—Ç–∞—É“ì–∞ –±–æ–ª–∞ –º–∞?\n",
      "- –ï–≥–µ—Ä –ø–µ–¥–∞–≥–æ–≥—Ç—ã“£ –µ“£–±–µ–∫ ”©—Ç—ñ–ª—ñ 30 –∂—ã–ª–¥–∞–Ω –∞—Å—Å–∞, –æ–ª –º—ñ–Ω–¥–µ—Ç—Ç—ñ –ü–ë–ë-–¥–µ–Ω ”©—Ç—É—ñ —Ç–∏—ñ—Å –ø–µ?\n",
      "- –ï–≥–µ—Ä –ø–µ–¥–∞–≥–æ–≥—Ç—ã“£ –¥–∏–ø–ª–æ–º—ã–Ω–¥–∞ –µ–∫—ñ –ø”ô–Ω –∫”©—Ä—Å–µ—Ç—ñ–ª—Å–µ, –æ–ª “õ–∞–π –ø”ô–Ω –±–æ–π—ã–Ω—à–∞ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è–¥–∞–Ω ”©—Ç—É—ñ –∫–µ—Ä–µ–∫?\n",
      "- –ü–µ–¥–∞–≥–æ–≥ –º–µ–∫—Ç–µ–ø–∫–µ –¥–µ–π—ñ–Ω–≥—ñ “±–π—ã–º–Ω–∞–Ω –æ—Ä—Ç–∞ –±—ñ–ª—ñ–º –±–µ—Ä—É “±–π—ã–º—ã–Ω–∞ –∞—É—ã—Å“õ–∞–Ω –∂–∞“ì–¥–∞–π–¥–∞ –æ–Ω—ã“£ –±—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Å–∞–Ω–∞—Ç—ã —Å–∞“õ—Ç–∞–ª–∞ –º–∞?\n",
      "- –ü–ë–ë “Ø—à—ñ–Ω ”©—Ç—É —à–µ–≥—ñ “õ–∞–Ω–¥–∞–π?\n",
      "- –ë—ñ–ª—ñ–∫—Ç—ñ–ª—ñ–∫ —Ç–µ—Å—Ç—ñ–Ω–µ–Ω ”©—Ç—É “Ø—à—ñ–Ω –ø–µ–¥–∞–≥–æ–≥-–º–æ–¥–µ—Ä–∞—Ç–æ—Ä “õ–∞–Ω—à–∞ –±–∞–ª–ª –∂–∏–Ω–∞—É—ã –∫–µ—Ä–µ–∫?\n"
     ]
    }
   ],
   "source": [
    "for query in example_queries_kk:\n",
    "    print(f\"- {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fab07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f1f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspaces/chatbot-rag-83/documents/documents.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b42f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39dcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data.items():\n",
    "    new_key = key + '‚Ññ 83 –û–± —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ –ü—Ä–∞–≤–∏–ª –∏ —É—Å–ª–æ–≤–∏–π –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ –∏ –ø—Ä–∏—Ä–∞–≤–Ω–µ–Ω–Ω—ã—Ö –∫ –Ω–∏–º –ª–∏—Ü, –∑–∞–Ω–∏–º–∞—é—â–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è—Ö –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, —Ä–µ–∞–ª–∏–∑—É—é—â–∏—Ö –æ–±—â–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —É—á–µ–±–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–æ—à–∫–æ–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø–∏—Ç–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ, –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ –æ–±—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ, –ø–æ—Å–ª–µ—Å—Ä–µ–¥–Ω–µ–≥–æ, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —É—á–µ–±–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã, –∏–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏—Ö —Å–ª—É–∂–∞—â–∏—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ '\n",
    "    new_data[new_key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6f043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspaces/chatbot-rag-83/documents/documents.json', 'w') as f:\n",
    "    json.dump(new_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c89fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ 14: –õ–∏—Å—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —É—Ä–æ–∫–∞ (–∑–∞–Ω—è—Ç–∏—è, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ–∏ÃÜ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è) –ø–µ–¥–∞–≥–æ–≥–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ), –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ, –ø–æ—Å–ª–µ—Å—Ä–µ–¥–Ω–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ÃÜ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–µ—Ç–µ–∏ÃÜ-—Å–∏—Ä–æ—Ç –∏ –¥–µ—Ç–µ–∏ÃÜ, –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –±–µ–∑ –ø–æ–ø–µ—á–µ–Ω–∏—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
